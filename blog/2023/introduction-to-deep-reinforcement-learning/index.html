<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Introduction to Deep Reinforcement Learning | Ahad Jawaid </title> <meta name="author" content="Ahad Jawaid"> <meta name="description" content="My notes on Deep Reinforcement Learning (DRL) based on the first chapter of the 'Grokking Deep Reinforcement Learning' by Miguel Morales."> <meta name="keywords" content="Machine LearningArtificial IntelligenceAhadJawaidAhad JawaidReinforcement LearningDeep Learning"> <meta property="og:site_name" content="Ahad Jawaid"> <meta property="og:type" content="article"> <meta property="og:title" content="Ahad Jawaid | Introduction to Deep Reinforcement Learning"> <meta property="og:url" content="https://www.ahadjawaid.com/blog/2023/introduction-to-deep-reinforcement-learning/"> <meta property="og:description" content="My notes on Deep Reinforcement Learning (DRL) based on the first chapter of the 'Grokking Deep Reinforcement Learning' by Miguel Morales."> <meta property="og:image" content="assets/img/prof_pic.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Introduction to Deep Reinforcement Learning"> <meta name="twitter:description" content="My notes on Deep Reinforcement Learning (DRL) based on the first chapter of the 'Grokking Deep Reinforcement Learning' by Miguel Morales."> <meta name="twitter:image" content="assets/img/prof_pic.png"> <meta name="twitter:site" content="@ahadj0"> <meta name="twitter:creator" content="@ahadj0"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Ahad Jawaid"
        },
        "url": "https://www.ahadjawaid.com/blog/2023/introduction-to-deep-reinforcement-learning/",
        "@type": "BlogPosting",
        "description": "My notes on Deep Reinforcement Learning (DRL) based on the first chapter of the 'Grokking Deep Reinforcement Learning' by Miguel Morales.",
        "headline": "Introduction to Deep Reinforcement Learning",
        
        "sameAs": ["https://scholar.google.com/citations?user=IKtwsQ8AAAAJ", "https://github.com/ahadjawaid", "https://www.linkedin.com/in/ahad-jawaid", "https://twitter.com/ahadj0"],
        
        "name": "Ahad Jawaid",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%A1&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.ahadjawaid.com/blog/2023/introduction-to-deep-reinforcement-learning/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Ahad Jawaid </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Introduction to Deep Reinforcement Learning</h1> <p class="post-meta"> Created in June 27, 2023 by Ahad Jawaid </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/category/deep-reinforcement-learning"> <i class="fa-solid fa-tag fa-sm"></i> Deep Reinforcement Learning</a>   <a href="/blog/category/notes"> <i class="fa-solid fa-tag fa-sm"></i> Notes</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <style>.responsive-img{width:100%}@media(min-width:992px){.responsive-img{width:75%}}</style> <h2 id="artificial-intelligence-ai">Artificial Intelligence (AI)</h2> <ul> <li>Artificial Intelligence (AI) is a domain of computer science dedicated to developing software capable of exhibiting attributes of intelligence.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/subfields_of_artificial_intelligence.jpg" sizes="95vw"></source> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/subfields_of_artificial_intelligence.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="machine-learning-ml">Machine Learning (ML)</h2> <ul> <li>Machine learning, a subset of AI, tackles problems necessitating intelligent solutions by learning from data.</li> <li> <strong>Supervised Learning (SL):</strong> A method that learns from labeled data. <ul> <li>E.g., handwritten-digit-recognition</li> </ul> </li> <li> <strong>Unsupervised Learning (UL):</strong> A method that learns from unlabeled data <ul> <li>E.g., customer segmentaiton</li> </ul> </li> <li> <strong>Reinforcement Learning (RL):</strong> A method that learns from trial and error <ul> <li>E.g., pong-playing agent</li> </ul> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/main_branches_of_machine_learning.jpg" sizes="95vw"></source> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/main_branches_of_machine_learning.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="deep-learning-dl">Deep Learning (DL)</h2> <ul> <li>Deep Learning employs multi-layered non-linear function approximations, also known as neural networks, to address ML tasks. Essentially, it is a suite of techniques that utilize neural networks to solve ML challenges.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/deep_learning_is_a_powerful_toolbox.jpg" sizes="95vw"></source> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/deep_learning_is_a_powerful_toolbox.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="deep-reinforcement-learning-drl">Deep Reinforcement Learning (DRL)</h3> <ul> <li>Deep Reinforcement Learning learns through trial and error from feedback that’s simultaneously sequentially, evaluative, and sampled by leveraging non-linear function approximation (neural networks).</li> </ul> <h2 id="reinforcement-learning-rl">Reinforcement Learning (RL)</h2> <h3 id="similar-fields">Similar fields</h3> <ul> <li> <strong>Reinforcement Learning (RL):</strong> Investigates methods of resolving complex sequential decision-making problems under uncertain conditions.</li> <li> <strong>Control Theory (CT)</strong>: Examines methods of controlling complex known dynamic systems.</li> <li> <strong>Operations Research (OR)</strong>: Investigates decision-making under uncertain conditions, generally featuring a larger action space than in DRL.</li> <li> <strong>Psychology</strong>: Studies human behavior, which frequently encapsulates complex sequential decision-making problems under uncertainty.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/the_synergy_between_similar_fields.jpg" sizes="95vw"></source> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/the_synergy_between_similar_fields.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="agent-and-enviroment">Agent and Enviroment</h3> <ul> <li> <p><strong>Agent:</strong> Refers exclusively to the decision-making entity.</p> <ul> <li>For instance, if you are training a robot arm to pick up a toy, the agent is the code that makes the decisions, not the robot arm itself.</li> </ul> </li> <li> <p><strong>Environment:</strong> Includes everything external to the agent, beyond the agent’s control, and everything that follows the agent’s decisions.</p> <ul> <li>In the context of training a robot arm to pick up a toy, the objects to be picked up, the tray where the objects reside, atmospheric conditions like wind, and even the robot arm itself are all considered parts of the environment.</li> </ul> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/boundary_between_agent_and_environment.jpg" sizes="95vw"></source> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/boundary_between_agent_and_environment.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="states-and-observations">States and Observations</h3> <ul> <li> <strong>State Space:</strong> The set of all possible variables and values that can represent the state of the environment.</li> <li> <strong>State:</strong> A comprehensive description of the environment, or an instantiation of the state space.</li> <li> <strong>Observation:</strong> A partial or incomplete description of the environment.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/states_vs_observations.jpg" sizes="95vw"></source> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/states_vs_observations.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="reinforcement-learning-cycle">Reinforcement Learning Cycle</h3> <ul> <li> <strong>Transition Function:</strong> The mapping from the agent’s action to a potentially new state.</li> <li> <strong>Reward Function:</strong> The mapping from the action taken to the potential reward signal. <ul> <li>Goals are defined via the reward function.</li> </ul> </li> <li> <strong>Model:</strong> A set of the transitions and rewards.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/the_reinforcement_learning_cycle.jpg" sizes="95vw"></source> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/the_reinforcement_learning_cycle.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h4 id="agents-improvement-process">Agent’s Improvement process:</h4> <ol> <li>Interact with the environment.</li> <li>Evaluates its behavior.</li> <li>Improves its responses.</li> </ol> <h4 id="agents-can-be-designed-to-learn">Agent’s can be designed to learn:</h4> <ul> <li> <strong>Policy:</strong> The mapping from observations to actions.</li> <li> <strong>Models:</strong> A model of the environment on mappings.</li> <li> <strong>Value Functions:</strong> The mapping of a state to its estimated value.</li> </ul> <h3 id="experiences">Experiences</h3> <ul> <li> <strong>Time Step:</strong> A single cycle of interaction between the agent and the environment.</li> <li> <strong>Experience:</strong> The set consisting of the state, the action, the reward, and the new state in a single time step.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/experience_tuples.jpg" sizes="95vw"></source> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/experience_tuples.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <strong>Episodic Task:</strong> Tasks that have a natural ending or goes on finitely many step. <ul> <li>E.g., video games</li> </ul> </li> <li> <strong>Continuing Task:</strong> Tasks that don’t have a natural ending or could go on indefinitely. <ul> <li>E.g., learning forward motion</li> </ul> </li> </ul> <h3 id="credit-assignment-problem">Credit Assignment Problem</h3> <ul> <li> <strong>Temporal Credit Assignment Problem:</strong> the challenge in determining which state and/or action is responsible for a reward the agent recieves <ul> <li>Usually occurs when the agent may have delayed rewards from an action or state that caused it hence the temporal aspect of the problem</li> </ul> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/temporal_credit_assignment_problem.jpg" sizes="95vw"></source> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/temporal_credit_assignment_problem.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="exploration-vs-exploitation">Exploration vs. Exploitation</h3> <ul> <li> <strong>Evaluative Feedback:</strong> Feedback that provides an indication of performance but not correctness.</li> <li> <strong>Exploration versus Exploitation trade-off:</strong> The balance between collecting new information from the environment and using known information to maximize rewards.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/exploration_vs_exploitation.jpg" sizes="95vw"></source> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/exploration_vs_exploitation.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="sampled-feedback">Sampled Feedback</h3> <ul> <li>Learning from sparse or weak feedback becomes more challenging with samples only. The agent must be capable of generalizing to learn from sampled feedback.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/learning_from_sampled_feedback.jpg" sizes="95vw"></source> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/learning_from_sampled_feedback.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="types-of-agents">Types of Agents</h3> <ul> <li> <strong>Policy-based:</strong> Designed to approximate policies.</li> <li> <strong>Model-based:</strong> Designed to approximate models.</li> <li> <strong>Value-based:</strong> Designed to approximate value functions.</li> <li> <strong>Actor-critic:</strong> Designed to approximate both policies and value functions.</li> </ul> <h3 id="pros-and-cons">Pros and Cons</h3> <p><strong>Strength:</strong> Reinforcement learning excels in mastering specific tasks.</p> <p><strong>Weaknesses:</strong> To learn a well-performing policy, it generally requires millions of samples.</p> <h2 id="history-of-deep-reinforcement-learning">History of Deep Reinforcement Learning</h2> <ul> <li> <p><strong>Alan Turing - 1930:</strong> Developed the Turing Test, a test of a machine’s ability to exhibit intelligent behavior indistinguishable from that of a human.</p> </li> <li> <p><strong>John McCarthy - 1955:</strong> Coined the term “Artificial Intelligence”.</p> </li> <li> <p><strong>Andrew Ng - 2002:</strong> Trained an autonomous helicopter to perform stunts by observing human-expert flights using inverse reinforcement learning.</p> </li> <li> <p><strong>Nate Kohl and Peter Stone - 2002:</strong> Applied policy-gradient methods to train a soccer-playing robot.</p> </li> <li> <p><strong>Mnih et al. - 2013, 2015:</strong> Introduced the DQN algorithm, which learned to play Atari games from raw pixels.</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/atari_dqn.jpg" sizes="95vw"></source> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/atari_dqn.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>*</p> <ul> <li> <p><strong>Silver et al. - 2014:</strong> Released the deterministic policy gradient (DPG) algorithm.</p> </li> <li> <p><strong>Lillicrap et al. - 2015:</strong> Improved DPG with deep deterministic policy gradient (DDPG)</p> </li> <li> <p><strong>Schulman et al. - 2016:</strong> Released trust region policy optimization (TRPO) and generalized advantage estimation (GAE) methods.</p> </li> <li> <p><strong>Sergey Levine et al. - 2016:</strong> Published Guided Policy Search (GPS)</p> </li> <li> <p><strong>Silver et al. - 2016:</strong> Demonstrated AlphaGo</p> </li> <li> <p>…</p> </li> </ul> <h2 id="references">References</h2> <p>Morales, M. (2020). Grokking Deep Reinforcement Learning. Originally Published: October 15, 2020.</p> <p><em>All figures are sourced from this book.</em></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/personal-statement/">Writing a Personal Statement</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/nsf-grfp/">Applying to the 2025 NSF Graduate Fellowship</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/modeling-reinforcement-learning-problem/">Modeling the Reinforcement Learning Problem</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/translating-theory-into-code/">From Paper to Code: A Guide to Implement Research</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/the-universal-function/">Neural network: The universal function</a> </li> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Ahad Jawaid. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: November 03, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-Q9WDH7ENSQ"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-Q9WDH7ENSQ");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>