<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <style>.responsive-img{width:100%}@media(min-width:992px){.responsive-img{width:75%}}</style> </head> <body> <h2 id="artificial-intelligence-ai">Artificial Intelligence (AI)</h2> <ul> <li>Artificial Intelligence (AI) is a domain of computer science dedicated to developing software capable of exhibiting attributes of intelligence.</li> </ul> <p><img src="introduction-to-deep-reinforcement-learning/assets/subfields_of_artificial_intelligence.jpg" class="responsive-img"></p> <h2 id="machine-learning-ml">Machine Learning (ML)</h2> <ul> <li>Machine learning, a subset of AI, tackles problems necessitating intelligent solutions by learning from data.</li> <li> <strong>Supervised Learning (SL):</strong> A method that learns from labeled data. <ul> <li>E.g., handwritten-digit-recognition</li> </ul> </li> <li> <strong>Unsupervised Learning (UL):</strong> A method that learns from unlabeled data <ul> <li>E.g., customer segmentaiton</li> </ul> </li> <li> <strong>Reinforcement Learning (RL):</strong> A method that learns from trial and error <ul> <li>E.g., pong-playing agent</li> </ul> </li> </ul> <p><img src="introduction-to-deep-reinforcement-learning/assets/main_branches_of_machine_learning.jpg" class="responsive-img"></p> <h2 id="deep-learning-dl">Deep Learning (DL)</h2> <ul> <li>Deep Learning employs multi-layered non-linear function approximations, also known as neural networks, to address ML tasks. Essentially, it is a suite of techniques that utilize neural networks to solve ML challenges.</li> </ul> <p><img src="introduction-to-deep-reinforcement-learning/assets/deep_learning_is_a_powerful_toolbox.jpg" class="responsive-img"></p> <h3 id="deep-reinforcement-learning-drl">Deep Reinforcement Learning (DRL)</h3> <ul> <li>Deep Reinforcement Learning learns through trial and error from feedback that’s simultaneously sequentially, evaluative, and sampled by leveraging non-linear function approximation (neural networks).</li> </ul> <h2 id="reinforcement-learning-rl">Reinforcement Learning (RL)</h2> <h3 id="similar-fields">Similar fields</h3> <ul> <li> <strong>Reinforcement Learning (RL):</strong> Investigates methods of resolving complex sequential decision-making problems under uncertain conditions.</li> <li> <strong>Control Theory (CT)</strong>: Examines methods of controlling complex known dynamic systems.</li> <li> <strong>Operations Research (OR)</strong>: Investigates decision-making under uncertain conditions, generally featuring a larger action space than in DRL.</li> <li> <strong>Psychology</strong>: Studies human behavior, which frequently encapsulates complex sequential decision-making problems under uncertainty.</li> </ul> <p><img src="introduction-to-deep-reinforcement-learning/assets/the_synergy_between_similar_fields.jpg" class="responsive-img"></p> <h3 id="agent-and-enviroment">Agent and Enviroment</h3> <ul> <li> <strong>Agent:</strong> Refers exclusively to the decision-making entity. <ul> <li>For instance, if you are training a robot arm to pick up a toy, the agent is the code that makes the decisions, not the robot arm itself.</li> </ul> </li> <li> <strong>Environment:</strong> Includes everything external to the agent, beyond the agent’s control, and everything that follows the agent’s decisions. <ul> <li>In the context of training a robot arm to pick up a toy, the objects to be picked up, the tray where the objects reside, atmospheric conditions like wind, and even the robot arm itself are all considered parts of the environment.</li> </ul> </li> </ul> <p><img src="introduction-to-deep-reinforcement-learning/assets/boundary_between_agent_and_environment.jpg" class="responsive-img"></p> <h3 id="states-and-observations">States and Observations</h3> <ul> <li> <strong>State Space:</strong> The set of all possible variables and values that can represent the state of the environment.</li> <li> <strong>State:</strong> A comprehensive description of the environment, or an instantiation of the state space.</li> <li> <strong>Observation:</strong> A partial or incomplete description of the environment.</li> </ul> <p><img src="introduction-to-deep-reinforcement-learning/assets/states_vs_observations.jpg" class="responsive-img"></p> <h3 id="reinforcement-learning-cycle">Reinforcement Learning Cycle</h3> <ul> <li> <strong>Transition Function:</strong> The mapping from the agent’s action to a potentially new state.</li> <li> <strong>Reward Function:</strong> The mapping from the action taken to the potential reward signal. <ul> <li>Goals are defined via the reward function.</li> </ul> </li> <li> <strong>Model:</strong> A set of the transitions and rewards.</li> </ul> <p><img src="introduction-to-deep-reinforcement-learning/assets/the_reinforcement_learning_cycle.jpg" class="responsive-img"></p> <h4 id="agents-improvement-process">Agent’s Improvement process:</h4> <ol> <li>Interact with the environment.</li> <li>Evaluates its behavior.</li> <li>Improves its responses.</li> </ol> <h4 id="agents-can-be-designed-to-learn">Agent’s can be designed to learn:</h4> <ul> <li> <strong>Policy:</strong> The mapping from observations to actions.</li> <li> <strong>Models:</strong> A model of the environment on mappings.</li> <li> <strong>Value Functions:</strong> The mapping of a state to its estimated value.</li> </ul> <h3 id="experiences">Experiences</h3> <ul> <li> <strong>Time Step:</strong> A single cycle of interaction between the agent and the environment.</li> <li> <strong>Experience:</strong> The set consisting of the state, the action, the reward, and the new state in a single time step.</li> </ul> <p><img src="introduction-to-deep-reinforcement-learning/assets/experience_tuples.jpg" class="responsive-img"></p> <ul> <li> <strong>Episodic Task:</strong> Tasks that have a natural ending or goes on finitely many step. <ul> <li>E.g., video games</li> </ul> </li> <li> <strong>Continuing Task:</strong> Tasks that don’t have a natural ending or could go on indefinitely. <ul> <li>E.g., learning forward motion</li> </ul> </li> </ul> <h3 id="credit-assignment-problem">Credit Assignment Problem</h3> <ul> <li> <strong>Temporal Credit Assignment Problem:</strong> the challenge in determining which state and/or action is responsible for a reward the agent recieves <ul> <li>Usually occurs when the agent may have delayed rewards from an action or state that caused it hence the temporal aspect of the problem</li> </ul> </li> </ul> <p><img src="introduction-to-deep-reinforcement-learning/assets/temporal_credit_assignment_problem.jpg" class="responsive-img"></p> <h3 id="exploration-vs-exploitation">Exploration vs. Exploitation</h3> <ul> <li> <strong>Evaluative Feedback:</strong> Feedback that provides an indication of performance but not correctness.</li> <li> <strong>Exploration versus Exploitation trade-off:</strong> The balance between collecting new information from the environment and using known information to maximize rewards.</li> </ul> <p><img src="introduction-to-deep-reinforcement-learning/assets/exploration_vs_exploitation.jpg" class="responsive-img"></p> <h3 id="sampled-feedback">Sampled Feedback</h3> <ul> <li>Learning from sparse or weak feedback becomes more challenging with samples only. The agent must be capable of generalizing to learn from sampled feedback.</li> </ul> <p><img src="introduction-to-deep-reinforcement-learning/assets/learning_from_sampled_feedback.jpg" class="responsive-img"></p> <h3 id="types-of-agents">Types of Agents</h3> <ul> <li> <strong>Policy-based:</strong> Designed to approximate policies.</li> <li> <strong>Model-based:</strong> Designed to approximate models.</li> <li> <strong>Value-based:</strong> Designed to approximate value functions.</li> <li> <strong>Actor-critic:</strong> Designed to approximate both policies and value functions.</li> </ul> <h3 id="pros-and-cons">Pros and Cons</h3> <p><strong>Strength:</strong> Reinforcement learning excels in mastering specific tasks.</p> <p><strong>Weaknesses:</strong> To learn a well-performing policy, it generally requires millions of samples.</p> <h2 id="history-of-deep-reinforcement-learning">History of Deep Reinforcement Learning</h2> <ul> <li> <p><strong>Alan Turing - 1930:</strong> Developed the Turing Test, a test of a machine’s ability to exhibit intelligent behavior indistinguishable from that of a human.</p> </li> <li> <p><strong>John McCarthy - 1955:</strong> Coined the term “Artificial Intelligence”.</p> </li> <li> <p><strong>Andrew Ng - 2002:</strong> Trained an autonomous helicopter to perform stunts by observing human-expert flights using inverse reinforcement learning.</p> </li> <li> <p><strong>Nate Kohl and Peter Stone - 2002:</strong> Applied policy-gradient methods to train a soccer-playing robot.</p> </li> <li> <strong>Mnih et al. - 2013, 2015:</strong> Introduced the DQN algorithm, which learned to play Atari games from raw pixels. <ul> <li><img src="introduction-to-deep-reinforcement-learning/assets/atari_dqn.jpg" class="responsive-img"></li> </ul> </li> <li> <p><strong>Silver et al. - 2014:</strong> Released the deterministic policy gradient (DPG) algorithm.</p> </li> <li> <p><strong>Lillicrap et al. - 2015:</strong> Improved DPG with deep deterministic policy gradient (DDPG)</p> </li> <li> <p><strong>Schulman et al. - 2016:</strong> Released trust region policy optimization (TRPO) and generalized advantage estimation (GAE) methods.</p> </li> <li> <p><strong>Sergey Levine et al. - 2016:</strong> Published Guided Policy Search (GPS)</p> </li> <li> <p><strong>Silver et al. - 2016:</strong> Demonstrated AlphaGo</p> </li> <li>…</li> </ul> <h2 id="references">References</h2> <p>Morales, M. (2020). Grokking Deep Reinforcement Learning. Originally Published: October 15, 2020.</p> <p><em>All figures are sourced from this book.</em></p> </body> </html>