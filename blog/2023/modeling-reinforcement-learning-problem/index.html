<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Modeling the Reinforcement Learning Problem | Ahad Jawaid </title> <meta name="author" content="Ahad Jawaid"> <meta name="description" content="My notes on the second chapter of 'Grokking Deep Reinforcement Learning' by Miguel Morales. This post covers the components of the environment and how to model it using Markov Decision Processes (MDPs)."> <meta name="keywords" content="Machine LearningArtificial IntelligenceAhadJawaidAhad JawaidReinforcement LearningDeep Learning"> <meta property="og:site_name" content="Ahad Jawaid"> <meta property="og:type" content="article"> <meta property="og:title" content="Ahad Jawaid | Modeling the Reinforcement Learning Problem"> <meta property="og:url" content="https://www.ahadjawaid.com/blog/2023/modeling-reinforcement-learning-problem/"> <meta property="og:description" content="My notes on the second chapter of 'Grokking Deep Reinforcement Learning' by Miguel Morales. This post covers the components of the environment and how to model it using Markov Decision Processes (MDPs)."> <meta property="og:image" content="assets/img/prof_pic.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Modeling the Reinforcement Learning Problem"> <meta name="twitter:description" content="My notes on the second chapter of 'Grokking Deep Reinforcement Learning' by Miguel Morales. This post covers the components of the environment and how to model it using Markov Decision Processes (MDPs)."> <meta name="twitter:image" content="assets/img/prof_pic.png"> <meta name="twitter:site" content="@ahadj0"> <meta name="twitter:creator" content="@ahadj0"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Ahad Jawaid"
        },
        "url": "https://www.ahadjawaid.com/blog/2023/modeling-reinforcement-learning-problem/",
        "@type": "BlogPosting",
        "description": "My notes on the second chapter of 'Grokking Deep Reinforcement Learning' by Miguel Morales. This post covers the components of the environment and how to model it using Markov Decision Processes (MDPs).",
        "headline": "Modeling the Reinforcement Learning Problem",
        
        "sameAs": ["https://scholar.google.com/citations?user=IKtwsQ8AAAAJ", "https://github.com/ahadjawaid", "https://www.linkedin.com/in/ahad-jawaid", "https://twitter.com/ahadj0"],
        
        "name": "Ahad Jawaid",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%A1&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.ahadjawaid.com/blog/2023/modeling-reinforcement-learning-problem/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Ahad Jawaid </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Modeling the Reinforcement Learning Problem</h1> <p class="post-meta"> Created in July 01, 2023 by Ahad Jawaid </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/category/deep-reinforcement-learning"> <i class="fa-solid fa-tag fa-sm"></i> Deep Reinforcement Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>This post will discuss how to model the reinforcement learning problem using the mathematical framework of a Markov Decision Process (MDP). We’ll cover what an environment is in reinforcement learning, its components, and how to model it through an MDP.</p> <p>To understand what we are modeling, we must first familiarize ourselves with the components of reinforcement learning:</p> <h2 id="components-of-reinforcement-learning">Components of Reinforcement Learning</h2> <ul> <li> <em>Reinforcement Learning</em> is A solution for managing complex, uncertain sequential decision-making. <ul> <li> <em>Complex:</em> Pertains to the large scale of the state and action spaces that the agent needs to navigate.</li> <li> <em>Sequential:</em> Denotes the delayed effect of an agent’s actions, essentially the credit assignment problem.</li> <li> <em>Uncertainty:</em> Highlights the unpredictability of the impact of an agent’s actions on the environment, tying to balance the exploration vs exploitation trade-off.</li> </ul> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/reinforcement-learning-interaction-cycle.jpg" sizes="95vw"></source> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/reinforcement-learning-interaction-cycle.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="the-agent-the-decision-maker">The agent: The decision maker</h3> <ul> <li> <strong>Agents:</strong> These are entities solely responsible for making decisions that may influence the environment.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/internal-steps.jpg" sizes="95vw"></source> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/internal-steps.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <strong>Agent’s Improvement Process:</strong> <ol> <li>Interact</li> <li>Evaluate</li> <li>Improve</li> </ol> </li> </ul> <h3 id="the-environment-everything-else">The environment: Everything else</h3> <ul> <li>The <em>environment</em> symbolizes the problem at hand. It encapsulates everything external to the agent, anything that’s beyond the agent’s control but responds to the agent’s decisions.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/environment-process.jpg" sizes="95vw"></source> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/environment-process.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Pay attention to how we try to emulate the possible reactions of the environment to the agent’s actions.</p> <h2 id="unraveling-the-environment">Unraveling the Environment:</h2> <ul> <li>The environment is made up of states, actions, transition probabilities, and a reward function.</li> </ul> <h3 id="states-specific-configurations-of-the-environment">States: Specific configurations of the environment</h3> <ul> <li> <p>A <em>state</em> provides a comprehensive description of the environment.</p> </li> <li> <p><strong>State Space:</strong> A combiniation of all possible variables and values that can characterize the environment.</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/state-space.jpg" sizes="95vw"></source> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/state-space.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong>Initial States:</strong> A subset of states where the agent starts in the environment.</p> </li> <li> <p><strong>Terminal States:</strong> The final state in an episodic task, any transition from this state will leads back to itself.</p> </li> <li> <p><strong>Observation:</strong> The set of variables that the agent perceives. It might not be as comprehensive as a state.</p> </li> <li> <p><strong>Observation Space:</strong> All possible values of the variables observed by the agent.</p> </li> </ul> <h3 id="actions-a-mechanism-to-influence-the-environment">Actions: A mechanism to influence the environment</h3> <ul> <li> <p>An <em>action</em> signifies a choice that an agent can make to potentially alter the environment. The set of actions available to an agent may vary across states and during the agent’s learning process.</p> </li> <li> <p><strong>Action Space:</strong> The set of all actions in all states.</p> </li> </ul> <h3 id="transition-function-consequences-of-agent-actions">Transition Function: Consequences of agent actions</h3> <ul> <li>A <em>transition function</em> determines how the environment changes in response to an action.</li> <li>Denoted as $T(s, a, s’)$, where $s$ is the current state, $a$ is the action taken, and $s’$ is the resulting state.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/transition-function.jpg" sizes="95vw"></source> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/transition-function.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong>Stochastic Transitions:</strong> Transitions that don’t guarantee a single resulting state but could lead to multiple states, each with a different probability.</p> </li> <li> <p>In RL, we often assume that transition distributions remain stationary, whether they’re stochastic or deterministic. While this assumption can be relaxed to some degree, for most agents, the transitions must at least appear stationary.</p> </li> </ul> <h3 id="reward-function">Reward Function</h3> <ul> <li> <p>A <em>reward function</em> steers the agent towards its goal. It maps a state or an action to a scalar reward, indicative of the goodness but not necessarily the correctness of the agent’s state or action.</p> </li> <li> <p>Reward signals supervise your agent. A denser reward signal leads to quicker learning but imposes more bias on the agent’s task-solving approach. Conversely, a sparser (less frequent) reward signal results in a less biased agent, potentially leading to novel, emergent behavior.</p> </li> <li> <p><strong>Return:</strong> The sum of the rewards collected in a single episode.</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/reward-function.jpg" sizes="95vw"></source> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/reward-function.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="common-types-of-environments">Common Types of Environments:</h3> <ul> <li> <p><strong>Grid-World Environment:</strong> An environment that is a square grid of any size.</p> <ul> <li> <strong>Walk / Corridor:</strong> A type of grid-world environment with a single row.</li> </ul> </li> <li> <p><strong>Bandit Environment:</strong> An environment with a single non-terminal state, named ‘bandit’ as an analogy to a slot machine that will empty your pockets the same way bandits do.</p> </li> </ul> <h4 id="ways-to-represent-an-environment">Ways to represent an environment:</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/bandit-walk.jpg" sizes="95vw"></source> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/bandit-walk.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/bandit-walk-graph.jpg" sizes="95vw"></source> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/bandit-walk-graph.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Table Form</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/bandit-walk-table.jpg" sizes="95vw"></source> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/bandit-walk-table.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="markov-decision-process-mdps-the-engine-of-the-environment">Markov Decision Process (MDPs): The engine of the environment</h2> <ul> <li> <p><em>Markov Decision Processes (MDPs)</em> is a mathematical framework for modeling complex decision-making problems under uncertainty. It consists of system states, per-state actions, a transition function, a reward signal, a horizon, a discount factor, and an initial state distribution.</p> </li> <li> <p>In RL, it’s often assumed that all environments operate based on an underlying MDP, whether this assumption holds true or not.</p> </li> </ul> <h3 id="markov-property">Markov Property</h3> <ul> <li>For a state to have the <em>markov property</em>, it must contain all necessary variables to make it independent of all other states.</li> <li>More formally, the probability of the next state, given the current state and action, is independent of the history of interactions.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/markov-property.jpg" sizes="95vw"></source> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/markov-property.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>The markov assumption is useful as it allows us to keep the number of variables in a state to a minimum since the more variables you add, the longer it will take the agent to learn. However, completely adhering to the markov assumption may be impractical or even impossible.</li> </ul> <h3 id="horizon-time-changes-whats-optimal">Horizon: Time changes what’s optimal</h3> <ul> <li>A <em>time step</em> is a global clock synchronizing all parties and discretizing time.</li> <li> <p>An <em>episode</em> is a sequence of consecutive time steps from the start to the end of an episodic task.</p> </li> <li> <strong>Planning Horizon:</strong> It’s the temporal depth the agent must plan for.</li> <li> <strong>Finite Horizon:</strong> It’s a planning horizon that terminates after a specific number of steps.</li> <li> <strong>Infite / Indefinite Horizon:</strong> It’s an unlimited planning horizon, assuming that the task can continue indefinitely.</li> <li> <strong>Greedy Horizon:</strong> It’s a planning horizon of a single time step.</li> </ul> <h3 id="discount-the-future-is-uncertain-value-it-less">Discount: The future is uncertain, value it less</h3> <ul> <li>To handle the possibility of infinite sequences of interactions, we must convey to the agent that immediate rewards are more valuable than future ones. This concept is handled by introducing a <em>discount factor</em> that reduces the value of future rewards, stabilizing the learning of the task.</li> <li>Moreover, the more we look into the future, the more uncertainty we accumulate. Introducing a discount factor discounts these highly uncertain future rewards, preventing them from affecting our value estimates significantly.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/effect-of-discount-factor.jpg" sizes="95vw"></source> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/effect-of-discount-factor.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>The discount factor, $\gamma$ or gamma, reduces the variance of the value prediction by considering future, more uncertain, rewards less than immediate ones, fostering urgency in the agent.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/discount-factor.jpg" sizes="95vw"></source> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/discount-factor.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="extensions-to-mdps">Extensions to MDPs</h3> <ul> <li> <strong>Partially observable Markov decision process (POMDP):</strong> When the agent can’t fully observe the environment state.</li> <li> <strong>Factored Markov decision process (FMDP):</strong> It compactly represents the transition and reward functions, enabling the representation of large MDPs.</li> <li> <strong>Continuous Markov decision process:</strong> When either time, action, state, or any combination of them are continuous.</li> <li> <strong>Relational Markov decision process (RMDP):</strong> It allows combining probabilistic and relational knowledge.</li> <li> <strong>Semi-Markov decision process (SMDP):</strong> It allows the inclusion of abstract actions that take multiple time steps to complete.</li> <li> <strong>Multi-agent Markov decision process (MMDP):</strong> It allows multiple agents in the same environment.</li> <li> <strong>Decentralized Markov decision process (Dec-MDP):</strong> It allows multiple agents to collaborate and maximize a common reward.</li> </ul> <h3 id="putting-it-all-together">Putting it all together</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/mdps-vs-pomdps.jpg" sizes="95vw"></source> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/mdps-vs-pomdps.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="references">References</h2> <p>Morales, M. (2020). Grokking Deep Reinforcement Learning. Originally Published: October 15, 2020.</p> <p><em>All figures are sourced from this book.</em></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/technical-interview/">An Approach to the Technical Coding Interivew</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/personal-statement/">Writing a Personal Statement</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/nsf-grfp/">Applying to the 2025 NSF Graduate Fellowship</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/introduction-to-deep-reinforcement-learning/">Introduction to Deep Reinforcement Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/translating-theory-into-code/">From Paper to Code: A Guide to Implement Research</a> </li> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Ahad Jawaid. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: November 11, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-Q9WDH7ENSQ"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-Q9WDH7ENSQ");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>