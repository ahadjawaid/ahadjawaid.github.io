<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Modeling the Reinforcement Learning Problem | Ahad Jawaid </title> <meta name="author" content="Ahad Jawaid"> <meta name="description" content="My notes on the second chapter of 'Grokking Deep Reinforcement Learning' by Miguel Morales. This post covers the components of the environment and how to model it using Markov Decision Processes (MDPs)."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%A1&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ahadjawaid.github.io/blog/2023/modeling-reinforcement-learning-problem/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Ahad Jawaid </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Modeling the Reinforcement Learning Problem</h1> <p class="post-meta"> Created in July 01, 2023 by Ahad Jawaid </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/category/deep-reinforcement-learning"> <i class="fa-solid fa-tag fa-sm"></i> Deep Reinforcement Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <style>.responsive-img{width:100%}@media(min-width:992px){.responsive-img{width:75%}}</style> <p>This post will discuss how to model the reinforcement learning problem using the mathematical framework of a Markov Decision Process (MDP). We’ll cover what an environment is in reinforcement learning, its components, and how to model it through an MDP.</p> <p>To understand what we are modeling, we must first familiarize ourselves with the components of reinforcement learning:</p> <h2 id="components-of-reinforcement-learning">Components of Reinforcement Learning</h2> <ul> <li> <em>Reinforcement Learning</em> is A solution for managing complex, uncertain sequential decision-making. <ul> <li> <em>Complex:</em> Pertains to the large scale of the state and action spaces that the agent needs to navigate.</li> <li> <em>Sequential:</em> Denotes the delayed effect of an agent’s actions, essentially the credit assignment problem.</li> <li> <em>Uncertainty:</em> Highlights the unpredictability of the impact of an agent’s actions on the environment, tying to balance the exploration vs exploitation trade-off.</li> </ul> </li> </ul> <p><img src="assets/img/posts/modeling-reinforcement-learning-problem/reinforcement-learning-interaction-cycle.jpg" class="responsive-img"></p> <h3 id="the-agent-the-decision-maker">The agent: The decision maker</h3> <ul> <li> <strong>Agents:</strong> These are entities solely responsible for making decisions that may influence the environment.</li> </ul> <p><img src="assets/img/posts/modeling-reinforcement-learning-problem/internal-steps.jpg" class="responsive-img"></p> <ul> <li> <strong>Agent’s Improvement Process:</strong> <ol> <li>Interact</li> <li>Evaluate</li> <li>Improve</li> </ol> </li> </ul> <h3 id="the-environment-everything-else">The environment: Everything else</h3> <ul> <li>The <em>environment</em> symbolizes the problem at hand. It encapsulates everything external to the agent, anything that’s beyond the agent’s control but responds to the agent’s decisions.</li> </ul> <p><img src="assets/img/posts/modeling-reinforcement-learning-problem/environment-process.jpg" class="responsive-img"></p> <p>Pay attention to how we try to emulate the possible reactions of the environment to the agent’s actions.</p> <h2 id="unraveling-the-environment">Unraveling the Environment:</h2> <ul> <li>The environment is made up of states, actions, transition probabilities, and a reward function.</li> </ul> <h3 id="states-specific-configurations-of-the-environment">States: Specific configurations of the environment</h3> <ul> <li> <p>A <em>state</em> provides a comprehensive description of the environment.</p> </li> <li> <p><strong>State Space:</strong> A combiniation of all possible variables and values that can characterize the environment.</p> </li> </ul> <p><img src="assets/img/posts/modeling-reinforcement-learning-problem/state-space.jpg" class="responsive-img"></p> <ul> <li> <p><strong>Initial States:</strong> A subset of states where the agent starts in the environment.</p> </li> <li> <p><strong>Terminal States:</strong> The final state in an episodic task, any transition from this state will leads back to itself.</p> </li> <li> <p><strong>Observation:</strong> The set of variables that the agent perceives. It might not be as comprehensive as a state.</p> </li> <li> <p><strong>Observation Space:</strong> All possible values of the variables observed by the agent.</p> </li> </ul> <h3 id="actions-a-mechanism-to-influence-the-environment">Actions: A mechanism to influence the environment</h3> <ul> <li> <p>An <em>action</em> signifies a choice that an agent can make to potentially alter the environment. The set of actions available to an agent may vary across states and during the agent’s learning process.</p> </li> <li> <p><strong>Action Space:</strong> The set of all actions in all states.</p> </li> </ul> <h3 id="transition-function-consequences-of-agent-actions">Transition Function: Consequences of agent actions</h3> <ul> <li>A <em>transition function</em> determines how the environment changes in response to an action.</li> <li>Denoted as $T(s, a, s’)$, where $s$ is the current state, $a$ is the action taken, and $s’$ is the resulting state.</li> </ul> <p><img src="assets/img/posts/modeling-reinforcement-learning-problem/transition-function.jpg" class="responsive-img"></p> <ul> <li> <p><strong>Stochastic Transitions:</strong> Transitions that don’t guarantee a single resulting state but could lead to multiple states, each with a different probability.</p> </li> <li> <p>In RL, we often assume that transition distributions remain stationary, whether they’re stochastic or deterministic. While this assumption can be relaxed to some degree, for most agents, the transitions must at least appear stationary.</p> </li> </ul> <h3 id="reward-function">Reward Function</h3> <ul> <li> <p>A <em>reward function</em> steers the agent towards its goal. It maps a state or an action to a scalar reward, indicative of the goodness but not necessarily the correctness of the agent’s state or action.</p> </li> <li> <p>Reward signals supervise your agent. A denser reward signal leads to quicker learning but imposes more bias on the agent’s task-solving approach. Conversely, a sparser (less frequent) reward signal results in a less biased agent, potentially leading to novel, emergent behavior.</p> </li> <li> <p><strong>Return:</strong> The sum of the rewards collected in a single episode.</p> </li> </ul> <p><img src="assets/img/posts/modeling-reinforcement-learning-problem/reward-function.jpg" class="responsive-img"></p> <h3 id="common-types-of-environments">Common Types of Environments:</h3> <ul> <li> <strong>Grid-World Environment:</strong> An environment that is a square grid of any size. <ul> <li> <strong>Walk / Corridor:</strong> A type of grid-world environment with a single row.</li> </ul> </li> <li> <strong>Bandit Environment:</strong> An environment with a single non-terminal state, named ‘bandit’ as an analogy to a slot machine that will empty your pockets the same way bandits do.</li> </ul> <h4 id="ways-to-represent-an-environment">Ways to represent an environment:</h4> <p><img src="assets/img/posts/modeling-reinforcement-learning-problem/bandit-walk.jpg" class="responsive-img"></p> <p><img src="assets/img/posts/modeling-reinforcement-learning-problem/bandit-walk-graph.jpg" class="responsive-img"></p> <p><strong>Table Form</strong></p> <p><img src="assets/img/posts/modeling-reinforcement-learning-problem/bandit-walk-table.jpg" class="responsive-img"></p> <h2 id="markov-decision-process-mdps-the-engine-of-the-environment">Markov Decision Process (MDPs): The engine of the environment</h2> <ul> <li> <p><em>Markov Decision Processes (MDPs)</em> is a mathematical framework for modeling complex decision-making problems under uncertainty. It consists of system states, per-state actions, a transition function, a reward signal, a horizon, a discount factor, and an initial state distribution.</p> </li> <li> <p>In RL, it’s often assumed that all environments operate based on an underlying MDP, whether this assumption holds true or not.</p> </li> </ul> <h3 id="markov-property">Markov Property</h3> <ul> <li>For a state to have the <em>markov property</em>, it must contain all necessary variables to make it independent of all other states.</li> <li>More formally, the probability of the next state, given the current state and action, is independent of the history of interactions.</li> </ul> <p><img src="assets/img/posts/modeling-reinforcement-learning-problem/markov-property.jpg" class="responsive-img"></p> <ul> <li>The markov assumption is useful as it allows us to keep the number of variables in a state to a minimum since the more variables you add, the longer it will take the agent to learn. However, completely adhering to the markov assumption may be impractical or even impossible.</li> </ul> <h3 id="horizon-time-changes-whats-optimal">Horizon: Time changes what’s optimal</h3> <ul> <li>A <em>time step</em> is a global clock synchronizing all parties and discretizing time.</li> <li> <p>An <em>episode</em> is a sequence of consecutive time steps from the start to the end of an episodic task.</p> </li> <li> <strong>Planning Horizon:</strong> It’s the temporal depth the agent must plan for.</li> <li> <strong>Finite Horizon:</strong> It’s a planning horizon that terminates after a specific number of steps.</li> <li> <strong>Infite / Indefinite Horizon:</strong> It’s an unlimited planning horizon, assuming that the task can continue indefinitely.</li> <li> <strong>Greedy Horizon:</strong> It’s a planning horizon of a single time step.</li> </ul> <h3 id="discount-the-future-is-uncertain-value-it-less">Discount: The future is uncertain, value it less</h3> <ul> <li>To handle the possibility of infinite sequences of interactions, we must convey to the agent that immediate rewards are more valuable than future ones. This concept is handled by introducing a <em>discount factor</em> that reduces the value of future rewards, stabilizing the learning of the task.</li> <li>Moreover, the more we look into the future, the more uncertainty we accumulate. Introducing a discount factor discounts these highly uncertain future rewards, preventing them from affecting our value estimates significantly.</li> </ul> <p><img src="assets/img/posts/modeling-reinforcement-learning-problem/effect-of-discount-factor.jpg" class="responsive-img"></p> <ul> <li>The discount factor, $\gamma$ or gamma, reduces the variance of the value prediction by considering future, more uncertain, rewards less than immediate ones, fostering urgency in the agent.</li> </ul> <p><img src="assets/img/posts/modeling-reinforcement-learning-problem/discount-factor.jpg" class="responsive-img"></p> <h3 id="extensions-to-mdps">Extensions to MDPs</h3> <ul> <li> <strong>Partially observable Markov decision process (POMDP):</strong> When the agent can’t fully observe the environment state.</li> <li> <strong>Factored Markov decision process (FMDP):</strong> It compactly represents the transition and reward functions, enabling the representation of large MDPs.</li> <li> <strong>Continuous Markov decision process:</strong> When either time, action, state, or any combination of them are continuous.</li> <li> <strong>Relational Markov decision process (RMDP):</strong> It allows combining probabilistic and relational knowledge.</li> <li> <strong>Semi-Markov decision process (SMDP):</strong> It allows the inclusion of abstract actions that take multiple time steps to complete.</li> <li> <strong>Multi-agent Markov decision process (MMDP):</strong> It allows multiple agents in the same environment.</li> <li> <strong>Decentralized Markov decision process (Dec-MDP):</strong> It allows multiple agents to collaborate and maximize a common reward.</li> </ul> <h3 id="putting-it-all-together">Putting it all together</h3> <p><img src="assets/img/posts/modeling-reinforcement-learning-problem/mdps-vs-pomdps.jpg" class="responsive-img"></p> <h2 id="references">References</h2> <p>Morales, M. (2020). Grokking Deep Reinforcement Learning. Originally Published: October 15, 2020.</p> <p><em>All figures are sourced from this book.</em></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/introduction-to-deep-reinforcement-learning/">Introduction to Deep Reinforcement Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/translating-theory-into-code/">From Paper to Code: A Guide to Implement Research</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/the-universal-function/">Neural network: The universal function</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Ahad Jawaid. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-Q9WDH7ENSQ"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-Q9WDH7ENSQ");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-teaching",title:"teaching",description:"Materials for courses you taught. Replace this text with your description.",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"post-modeling-the-reinforcement-learning-problem",title:"Modeling the Reinforcement Learning Problem",description:"My notes on the second chapter of &#39;Grokking Deep Reinforcement Learning&#39; by Miguel Morales. This post covers the components of the environment and how to model it using Markov Decision Processes (MDPs).",section:"Posts",handler:()=>{window.location.href="/blog/2023/modeling-reinforcement-learning-problem/"}},{id:"post-introduction-to-deep-reinforcement-learning",title:"Introduction to Deep Reinforcement Learning",description:"My notes on Deep Reinforcement Learning (DRL) based on the first chapter of the &#39;Grokking Deep Reinforcement Learning&#39; by Miguel Morales.",section:"Posts",handler:()=>{window.location.href="/blog/2023/introduction-to-deep-reinforcement-learning/"}},{id:"post-from-paper-to-code-a-guide-to-implement-research",title:"From Paper to Code: A Guide to Implement Research",description:"This guide covers effective reading, model implementation, code validation, and the power of repetition.",section:"Posts",handler:()=>{window.location.href="/blog/2023/translating-theory-into-code/"}},{id:"post-neural-network-the-universal-function",title:"Neural network: The universal function",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/the-universal-function/"}},{id:"news-won-3rd-place-in-the-toyota-challenge-at-wehack-2022",title:"Won 3rd place in the Toyota challenge at WeHack 2022",description:"",section:"News"},{id:"news-won-3rd-place-in-the-cbre-challenge-at-hackutd-ix",title:"Won 3rd place in the CBRE challenge at HackUTD IX",description:"",section:"News"},{id:"news-won-1st-place-in-the-patient-safety-challenge-at-hackutd-x",title:"Won 1st place in the Patient Safety challenge at HackUTD X",description:"",section:"News"},{id:"news-awarded-best-use-of-ai-in-education-at-tamuhack-x",title:"Awarded Best Use of AI in Education at TamuHack X",description:"",section:"News"},{id:"news-awarded-the-jonsson-school-undergraduate-research-award",title:"Awarded the Jonsson School Undergraduate Research Award",description:"",section:"News"},{id:"projects-eco",title:"Eco",description:"Support eco-friendly projects by funding projects.",section:"Projects",handler:()=>{window.location.href="/projects/eco/"}},{id:"projects-random-leetcode",title:"Random Leetcode",description:"A website that randomly select a leetcode question from the blind 75 list.",section:"Projects",handler:()=>{window.location.href="/projects/random-leetcode/"}},{id:"projects-ambient",title:"Ambient",description:"Analyze customer service calls to get an overview of your service quality and effectiveness.",section:"Projects",handler:()=>{window.location.href="/projects/ambient/"}},{id:"projects-letter-grade-calculator",title:"Letter Grade Calculator",description:"A script that let&#39;s you caclulate your class&#39;s letter grade from your scores and the weightings of you scores.",section:"Projects",handler:()=>{window.location.href="/projects/grade-calculator/"}},{id:"projects-robo-rev",title:"Robo Rev",description:"Train your own robotic dog companion with voice recognition and object detection abilities.",section:"Projects",handler:()=>{window.location.href="/projects/robo-rev/"}},{id:"projects-wavenet-implementation",title:"WaveNet Implementation",description:"An implementation of an unconditioned wavenet architecture.",section:"Projects",handler:()=>{window.location.href="/projects/wavenet/"}},{id:"projects-fintuned-speech-emotion-recognition",title:"Fintuned Speech Emotion Recognition",description:"Fine tuned a ResNet image classifer on the speech emotion recongition task.",section:"Projects",handler:()=>{window.location.href="/projects/ser/"}},{id:"projects-bookify",title:"Bookify",description:"Convert book and research paper titles to audio formats for easy listening. Perfect for busy individuals or those with visual impairments.",section:"Projects",handler:()=>{window.location.href="/projects/bookify/"}},{id:"projects-optimine",title:"Optimine",description:"A website that mined data from twitter for specific topics and analyzed their sentiments.",section:"Projects",handler:()=>{window.location.href="/projects/optimine/"}},{id:"projects-fastspeech-1-2",title:"Fastspeech 1 / 2",description:"A pytorch implementation of the FastSpeech architecture trained on the text-to-speech task.",section:"Projects",handler:()=>{window.location.href="/projects/fastspeech/"}},{id:"projects-simple-gridworld",title:"Simple Gridworld",description:"A gridworld used for training reinforcement learning algorithms using the openai&#39;s gym library.",section:"Projects",handler:()=>{window.location.href="/projects/gridworld/"}},{id:"projects-multi-armed-bandits",title:"Multi Armed Bandits",description:"A library contained Multi-Armed Bandit environments built ontop of openai&#39;s gym library.",section:"Projects",handler:()=>{window.location.href="/projects/mab/"}},{id:"projects-reinforcement-learning-algorithms",title:"Reinforcement Learning Algorithms",description:"A library filled with my implentation of reinforcement learning algorithms.",section:"Projects",handler:()=>{window.location.href="/projects/rl-algos/"}},{id:"projects-paper-leaderboard",title:"Paper Leaderboard",description:"We help you find the latest and most promising research papers through a community-driven leaderboard",section:"Projects",handler:()=>{window.location.href="/projects/paperleaderboard/"}},{id:"projects-create-your-demo",title:"Create Your Demo",description:"A studio to create voiceovers for demos.",section:"Projects",handler:()=>{window.location.href="/projects/createdemo/"}},{id:"projects-cs-resume-builder",title:"CS Resume Builder",description:"We give you personalized resume reviews to help you pass the resume screening process.",section:"Projects",handler:()=>{window.location.href="/projects/cs-resume/"}},{id:"projects-curis",title:"Curis",description:"Seeks to keep physicians informed about the best course of action for their patients. The project specifically focuses on Cancer patients and finds relevant medical trials for the patient.",section:"Projects",handler:()=>{window.location.href="/projects/curis/"}},{id:"projects-ddqn-for-atari",title:"DDQN for Atari",description:"Implemented the Double DQN (DDQN) reinforcement learning method for Atari environments in OpenAI Gym.",section:"Projects",handler:()=>{window.location.href="/projects/dqn-atari/"}},{id:"projects-discovery",title:"Discovery",description:"Teaching through discovery ficition generated on the fly using LLMs.",section:"Projects",handler:()=>{window.location.href="/projects/discovery/"}},{id:"projects-prescribe",title:"PreScribe",description:"Records a conversation between the doctor and the patient and transcribes it. While also giving a checklist of prescribed medications requirements to assist doctor in getting the correct medication history for each patient.",section:"Projects",handler:()=>{window.location.href="/projects/prescribe/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%61%68%61%64.%6A%61%77%61%69%64@%75%74%64%61%6C%6C%61%73.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=IKtwsQ8AAAAJ","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/ahad-jawaid","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>