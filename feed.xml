<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://www.ahadjawaid.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://www.ahadjawaid.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-11T15:58:09+00:00</updated><id>https://www.ahadjawaid.com/feed.xml</id><title type="html">Ahad Jawaid</title><subtitle>Hi! I&apos;m Ahad, and I&apos;m a software developer and a researcher who likes to build cool things :)</subtitle><entry><title type="html">Applying to the 2025 NSF Graduate Fellowship</title><link href="https://www.ahadjawaid.com/blog/2024/nsf-grfp/" rel="alternate" type="text/html" title="Applying to the 2025 NSF Graduate Fellowship"/><published>2024-11-03T00:00:00+00:00</published><updated>2024-11-03T00:00:00+00:00</updated><id>https://www.ahadjawaid.com/blog/2024/nsf-grfp</id><content type="html" xml:base="https://www.ahadjawaid.com/blog/2024/nsf-grfp/"><![CDATA[<p>The National Science Foundation (NSF) awards $159,000 to up to 2,300 Graduate Research Fellowships (GRFP) every year. In 2024, the NSF awarded fellowships to 14.44% of applicants [<a href="#references">1</a>]. This program is intended for U.S. citizens.</p> <p>I recently completed my application for the NSF GRFP and wanted to share the stuff I found important. I applied under the Computer Science - Robotics topic for the 2025 cycle.</p> <p>This post covers:</p> <ul> <li><a href="#nsf-fellowship-solicitation">NSF Fellowship Guidelines</a></li> <li><a href="#review-rubric">Grading Rubric</a></li> <li><a href="#discussions">Discussions</a></li> <li><a href="#statistics-for-the-program">Statistics for the Program</a></li> </ul> <h2 id="nsf-fellowship-solicitation">NSF Fellowship Solicitation</h2> <p>The most important resource for applying is the program’s <a href="https://new.nsf.gov/funding/opportunities/grfp-nsf-graduate-research-fellowship-program/nsf24-591/solicitation">solicitation</a> [<a href="#references">3</a>]. It outlines all essential information about the program, application guidelines, and goals.</p> <h3 id="program-goals">Program Goals</h3> <p>According to the solicitation [3], the goals of the program are to:</p> <ol> <li>Select individuals with the <strong>demonstrated potential</strong> to be <strong>high-achieving</strong> scientists and engineers.</li> <li><strong>Broaden participation</strong> of the full spectrum of diverse talents in STEM.</li> </ol> <h3 id="application-components">Application Components</h3> <p>The application includes:</p> <ul> <li><strong>Personal Statement</strong> <ul> <li>This statement should be max 3 pages. <a href="https://www.ahadjawaid.com/assets/pdf/nsf-grfp-personal-statement.pdf">My personal statement</a>.</li> </ul> </li> <li><strong>Research Statement</strong> <ul> <li>This statement should be max 2 pages.</li> </ul> </li> <li><strong>Reference Letters</strong></li> <li>Each letter is max 2 pages. You need Minimum 2 letters and can have a maximum of 5.</li> </ul> <p>Ensure you follow all formatting guidelines, as <strong>failure</strong> to do so can lead to <strong>automatic rejection</strong>.</p> <p>Each statement should address the intellectual merit and broader impacts of your proposed research. Here’s a breakdown of what to include in each statement:</p> <h3 id="intellectual-merit">Intellectual Merit:</h3> <p>This section should highlight your potential as a scientist or engineer, including research experience, academic achievements, and industry experience. It usually has more detail than the broader impacts section.</p> <h4 id="broader-impacts">Broader Impacts:</h4> <p>In the research statement, broader impacts should relate to your research. In the personal statement, they should focus on activities outside of your research. Address the societal benefits of your work, such as:</p> <ul> <li>Full participation of women, persons with disabilities, and underrepresented minorities in STEM.</li> <li>Improved STEM education and educator development at any level.</li> <li>Increased public scientific literacy and engagement with science and technology.</li> <li>Improved well-being of individuals in society.</li> <li>Development of a diverse, globally competitive STEM workforce.</li> <li>Increased partnerships between academia, industry, and other sectors.</li> <li>Improved national security.</li> <li>Increased economic competitiveness of the United States.</li> <li>Enhanced infrastructure for research and education.</li> </ul> <p>Reference letters should also discuss both intellectual merit and broader impacts.</p> <h2 id="review-rubric">Review Rubric</h2> <p>The review rubric for the NSF GRFP includes:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/nsf-grfp/rubric.png" sizes="95vw"/> <img src="/assets/img/posts/nsf-grfp/rubric.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The rubric comes from the solicitation [<a href="#references">3</a>], while the scoring system was explained during an NSF GRFP bootcamp at my university.</p> <h2 id="discussions">Discussions</h2> <p>Based on past applications found at <a href="https://www.alexhunterlang.com/nsf-fellowship">Alex Hunter Lang Website</a> [<a href="#references">4</a>], successful applicants often have at least one research publication.</p> <p>Writing my NSF GRFP application took about five months, juggling classes and research. I recommend starting early and refining your statements with feedback from peers. Clarity and reliability are critical.</p> <p>Feel free to reach out if you have questions.</p> <h2 id="statistics-for-the-program">Statistics for the Program</h2> <p>After applying, I analyzed the 2024 awardee statistics. Here’s a breakdown:</p> <h3 id="distribution-of-recipient-majors">Distribution of recipient majors:</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/nsf-grfp/nsf_grfp_major_distribution.png" sizes="95vw"/> <img src="/assets/img/posts/nsf-grfp/nsf_grfp_major_distribution.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="top-35-fields-accepted-in-the-nsf-grfp">Top 35 fields accepted in the NSF GRFP:</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/nsf-grfp/nsf-grfp-top-35-subfields.png" sizes="95vw"/> <img src="/assets/img/posts/nsf-grfp/nsf-grfp-top-35-subfields.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="top-computer-science-subfields-accepted">Top computer science subfields accepted:</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/nsf-grfp/nsf-grfp-cs-subfields.png" sizes="95vw"/> <img src="/assets/img/posts/nsf-grfp/nsf-grfp-cs-subfields.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="top-20-institutions-by-number-of-recipients">Top 20 institutions by number of recipients:</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/nsf-grfp/nsf-grfp-top-20-schools.png" sizes="95vw"/> <img src="/assets/img/posts/nsf-grfp/nsf-grfp-top-20-schools.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>These statistics were derived from public NSF data, accessible <a href="https://www.research.gov/grfp/AwardeeList.do?method=loadAwardeeList">here</a> [<a href="#references">2</a>].</p> <hr/> <h2 id="references">References</h2> <p>[1] <a href="https://www.nsf.gov/edu/GRFPFacts.jsp">https://www.nsf.gov/edu/GRFPFacts.jsp</a></p> <p>[2] <a href="https://www.research.gov/grfp/AwardeeList.do?method=loadAwardeeList">https://www.research.gov/grfp/AwardeeList.do?method=loadAwardeeList</a></p> <p>[3] <a href="https://new.nsf.gov/funding/opportunities/grfp-nsf-graduate-research-fellowship-program/nsf24-591/solicitation">https://new.nsf.gov/funding/opportunities/grfp-nsf-graduate-research-fellowship-program/nsf24-591/solicitation</a></p> <p>[4] <a href="https://www.alexhunterlang.com/nsf-fellowship">https://www.alexhunterlang.com/nsf-fellowship</a></p>]]></content><author><name>Ahad Jawaid</name></author><category term="Research"/><category term="Graduate School"/><category term="Fellowships"/><summary type="html"><![CDATA[An overview of the NSF Graduate Research Fellowship Program (GRFP) and key takeaways from my recent experience applying.]]></summary></entry><entry><title type="html">Writing a Personal Statement</title><link href="https://www.ahadjawaid.com/blog/2024/personal-statement/" rel="alternate" type="text/html" title="Writing a Personal Statement"/><published>2024-11-03T00:00:00+00:00</published><updated>2024-11-03T00:00:00+00:00</updated><id>https://www.ahadjawaid.com/blog/2024/personal-statement</id><content type="html" xml:base="https://www.ahadjawaid.com/blog/2024/personal-statement/"><![CDATA[<p>As students, we often need to write personal statements of purpose for university admissions and fellowship applications. Here, I wanted to share my experiences writing personal statements over the years. I’ve previously written personal statements for fellowships and university admissions.</p> <p>What helped me the most while writing these statements was actually seeing other people’s examples, so I’ve provided my statements from the past and links to websites that contain many examples:</p> <ul> <li><a href="https://www.ahadjawaid.com/assets/pdf/nsf-grfp-personal-statement.pdf">Personal statement for NSF Graduate Fellowship</a></li> <li><a href="https://docs.google.com/spreadsheets/d/1xoezGhbtcpg3BvNdag2F5dTQM-Xl2EELUgAfG1eUg0s">NSF Graduate Fellowship Personal Statements</a></li> </ul> <p>I’ll describe the approach I found logical for writing a personal statement. This may not be the best way for everyone, but it worked for me.</p> <h2 id="audience">Audience</h2> <p>Understanding the purpose of your writing is crucial, which means asking yourself: “What do I want my audience to get out of this?” For personal statements, this varies based on the audience. Depending on who they are, I emphasized different experiences to match the desired outcome.</p> <p>In fellowships, they often explicitly state what they’re looking for in a solicitation document. If none is available, contacting the fellowship officer for clarification is an option. For university admissions, you can reach out to departments to understand their selection criteria.</p> <h2 id="structure">Structure</h2> <p>The structure I used for my NSF Graduate Fellowship personal statement was:</p> <ol> <li><a href="#introduction">Introduction</a></li> <li><a href="#experiences">Intellectual Merit</a> (Experiences)</li> <li><a href="#broader-impacts">Broader Impacts</a></li> <li><a href="#future-goals">Future Goals</a></li> </ol> <p>For fellowships, a recommended structure is often provided, so refer to specific fellowship guidelines. Generally, the structure is designed to present a cohesive story that addresses all essential points.</p> <h2 id="introduction">Introduction</h2> <p>The introduction should draw the reader in, and many applicants do this through a personal anecdote. Here’s an example of the approach I used for the NSF fellowship:</p> <blockquote> <p>“python inputBankingInfo.py Running… Finished. And with that, it was done.” The perpetual twelve hours of weekly data entry into Excel spreadsheets had forever ended. As an assistant real estate manager, I was tasked with repetitive work, like data entry. Every day, I slogged through inputting tenants’ accounts payable into an Excel sheet, thinking there had to be a better way. That’s when I discovered <em>Automate the Boring Stuff with Python</em>, my first programming book, and automated the data entry process—marking the beginning of my journey as a programmer.</p> </blockquote> <p>Many successful introductions start with a story that engages the reader. For me, this example outlines how I got into programming. I’ve found that writing an introduction in a style you’re comfortable with is important; avoid forcing a tone that feels unnatural.</p> <p>Next, I used a time jump to bring my story to the present:</p> <blockquote> <p>Four short years later, I have written code for Amazon, one of the largest companies on the planet; taught hundreds of students about machine learning (ML) and artificial intelligence (AI); participated in hackathons across the country; researched synthetic human-like speech generation; now I’m working towards creating robots that can execute tasks defined in natural language, while also exploring ways to communicate with computers directly through our brains.</p> </blockquote> <p>I then addressed a potential weakness in my application:</p> <blockquote> <p>My journey into research wasn’t straightforward. I didn’t even know what it meant to do research until the summer before my third year of university, and I didn’t stick to a single lab throughout my undergraduate years. Although this may have resulted in fewer publications, it helped me discover my interests, become more independent as a scientist, and strengthened my conviction that I want to pursue a career in research.</p> </blockquote> <p>Here, I acknowledged my limited publication record—a common metric of research success—while framing my diverse experiences positively.</p> <h2 id="experiences">Experiences</h2> <p>The experiences section is usually simpler to write, and I found the STAR method effective. Commonly used in interviews, it helps in writing about experiences concisely:</p> <ul> <li><strong>Situation</strong>: Provide background.</li> <li><strong>Task</strong>: Explain what you were trying to accomplish.</li> <li><strong>Action</strong>: Describe the steps you took.</li> <li><strong>Result</strong>: Highlight the outcome.</li> </ul> <p>Here’s an example from my statement:</p> <ul> <li><strong>Situation</strong>: <blockquote> <p>During the summer before my junior year, I collaborated with Dr. Kangkook Jee in his System Security Lab at the University of Texas at Dallas (UTD).</p> </blockquote> </li> <li><strong>Task</strong>: <blockquote> <p>My role was to extend the Graph Neural Network (GNN) Explainer algorithm to work with heterogeneous graphs, enabling us to generate explanations about why the GNN classifier detected a program as malicious based on the corresponding heterogeneous resource interaction graph.</p> </blockquote> </li> <li><strong>Action</strong>: <blockquote> <p>After programming the algorithm, I validated its accuracy by applying it to a classifier trained on the MUTAG (Mutagenic Chemicals) dataset. I confirmed the reliability of the algorithm’s explanations by comparing the molecules it identified as most important for classification as a mutagenic compound to established scientific ground truths, finding a strong match with existing research.</p> </blockquote> </li> <li><strong>Result</strong>: <blockquote> <p>This work led to my open-source contribution of the algorithm to the Deep Graph Library, the second-largest GNN library, and the algorithm was later used in the lab’s research publication.</p> </blockquote> </li> </ul> <p>Sometimes, even with a strong structure, an experience may not fit well into your narrative. In such cases, it’s worth pursuing new experiences that better support your story.</p> <h3 id="writing-results">Writing Results</h3> <p>When writing about results, it’s crucial to articulate the impact of your actions clearly. This can include tangible outcomes, like speed improvements, cost savings, or research publications. But when tangible metrics aren’t available, you can still highlight what you learned or the skills you developed.</p> <p>For example, a common job experience, like being a cashier, can be written from the perspective of the impact you made. Instead of saying, “I worked as a cashier at Walmart for four months,” you could frame it as: “I helped process approximately $100,000 in transactions over a four-month period, ensuring smooth and accurate customer interactions.” This shifts the focus to the benefit you created for the employer, making your contribution clearer and more impactful.</p> <p>Being creative with how you present your results can make a big difference. However, it’s essential to stay truthful and not exaggerate your impact. The goal is to frame your achievements in a way that emphasizes your contributions without distorting reality.</p> <h2 id="broader-impacts">Broader Impacts</h2> <p>Most personal statements require a section demonstrating how you’ll benefit others or society. For the NSF, this involves broader impacts like broadening participation in STEM. This section follows a modified STAR approach, with an added focus on future plans (STARF):</p> <ul> <li><strong>Situation</strong>: <blockquote> <p>When I first began learning about machine learning, I found the mathematical concepts intimidating.</p> </blockquote> </li> <li><strong>Task</strong>: <blockquote> <p>This inspired me to help others by explaining these ideas in a more accessible way, only requiring a basic understanding of mathematics (such as understanding the slope of a line).</p> </blockquote> </li> <li><strong>Action</strong>: <blockquote> <p>To that end, I became the workshop lead for the Artificial Intelligence Society, a campus organization, where I developed and presented eleven workshops and led a team of four to create nine of those workshops. I particularly encouraged students from various backgrounds to attend my workshops, even if they didn’t have a background in computer science. These workshops covered areas such as Deep Learning, Computer Vision, Natural Language Processing, Model Deployment, and Reinforcement Learning.</p> </blockquote> </li> <li><strong>Results</strong>: <blockquote> <p>The largest event had around 300 participants, while on average, each workshop attracted 57 students. Our feedback data showed an average engagement rating of 4.1/5.0, with 96% of participants indicating they learned something new.</p> </blockquote> </li> <li><strong>Future</strong>: <blockquote> <p>Moving forward, I plan to continue educating students through workshops and YouTube videos, presenting AI techniques in a simple and accessible manner to foster a diverse and vibrant AI community.</p> </blockquote> </li> </ul> <p>This approach links past impacts to future plans, making them credible.</p> <h2 id="future-goals">Future Goals</h2> <p>The future goals section ties everything together. It should connect your introduction, experiences, and broader impacts, showing how the opportunity you’re applying for will help you achieve your objectives and contribute to the organization’s goals.</p> <ul> <li><strong>Objective</strong>: <blockquote> <p>After completing my master’s, I plan to continue my research in robotics by pursuing a PhD in Computer Science and later becoming a research scientist. In my research career, my primary goal is to develop autonomous robots capable of reliably performing a wide range of tasks defined by humans, using natural inputs like text.</p> </blockquote> </li> <li><strong>Experiences and Impacts</strong>: <blockquote> <p>My experiences in both research and industry have equipped me with the technical and interpersonal skills necessary for a successful research career. Additionally, my experiences running workshops and helping students find jobs have instilled in me a deep commitment to educating and assisting others.</p> </blockquote> </li> <li><strong>Support and Alignment</strong>: <blockquote> <p>Receiving the NSF Graduate Research Fellowship would enhance my future research by giving me the flexibility to explore areas I find both interesting and impactful, as well as the freedom to continue to help other students. Awarding the fellowship to me would enable the NSF to further its mission of advancing artificial intelligence in robotics and broadening participation in the tech field.</p> </blockquote> </li> </ul> <h2 id="writing-process">Writing Process</h2> <p>My writing process generally follows four steps:</p> <ol> <li><strong>Outline</strong>: I outline the structure and organize the story.</li> <li><strong>Dump the details</strong>: I dump all details into the sections of my outline.</li> <li><strong>Edit</strong>: I revise for readability and cohesion. This involves rereading sections or using AI tools for assistance—whatever works best.</li> <li><strong>Review</strong>: I have others review my writing for readability. Incorporating their feedback is important to ensure the writing flows smoothly and is easy to understand.</li> </ol> <p>If you have further questions, feel free to reach out to me.</p>]]></content><author><name>Ahad Jawaid</name></author><category term="Graduate School"/><category term="Fellowships"/><summary type="html"><![CDATA[An overview of how I write my personal statements.]]></summary></entry><entry><title type="html">An Approach to the Technical Coding Interivew</title><link href="https://www.ahadjawaid.com/blog/2024/technical-interview/" rel="alternate" type="text/html" title="An Approach to the Technical Coding Interivew"/><published>2024-11-03T00:00:00+00:00</published><updated>2024-11-03T00:00:00+00:00</updated><id>https://www.ahadjawaid.com/blog/2024/technical-interview</id><content type="html" xml:base="https://www.ahadjawaid.com/blog/2024/technical-interview/"><![CDATA[<p>I recently started preparing for interviews again, and through discussions with friends, I noticed how to approach technical interviews isn’t always clear. While there are many people with more experience interviewing at top companies than me, I thought I’d share my approach which has worked for me in interviews with Amazon, Goldman Sachs, Capital One, and Toyota.</p> <p>Technical interviews at big tech companies, particularly those modeled after “FAANG” (Facebook, Amazon, Apple, Netflix, Google), tend to follow a specific format. Often, you’re given a problem similar to a “LeetCode” esque question, with around 20 to 45 minutes to solve it. Some companies, like Meta, may even ask their applicants solving two questions within a 40 minutes interval.</p> <h3 id="my-formula-for-technical-coding-interviews">My Formula for Technical Coding Interviews</h3> <p>Here’s the process I typically follow during a technical interview:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/technical-interview/technical-interview.png" sizes="95vw"/> <img src="/assets/img/posts/technical-interview/technical-interview.png" class="img-fluid rounded z-depth-1 w-50 mx-auto" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h4 id="1-read-the-problem">1. Read the Problem</h4> <p>While reading the problem, I try not to go silent. Speaking out loud helps communicate my understanding to the interviewer. Here, it’s important for me to understand the relationship between the inputs and outputs.</p> <h4 id="2-clarify-assumptions">2. Clarify Assumptions</h4> <p>After understanding the intial prompt, I validate my assumptions by asking the interviewer. Holding an incorrect assumption throughout the interview can lead to solving the wrong problem. Asking questions can prevent misunderstandings and allows me to validate the problem requirements directly with the interviewer.</p> <h4 id="3-identify-test-cases-optional">3. Identify Test Cases (Optional)</h4> <p>If time permits, I write a few test cases to better understand the problem. For example, for a problem like “two-sum,” a test case I write might look like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array = [1, 2, 3, 4], target = 5 -&gt; [1, 2]
</code></pre></div></div> <p>Specifying the expected inputs and outputs clarifies my understanding, and I validate these cases are correct with the interviewer. These initial test cases also help verify my solution once it’s implemented.</p> <h4 id="4-list-potential-solutions">4. List Potential Solutions</h4> <p>Next, I start brainstorming solutions based on familiar patterns, such as “two-pointer” methods for arrays or hash maps for fast lookups. Initially, I might discuss a brute-force approach before working towards an optimized solution. When discussing each option, I evaluate its time and space complexity to justify my choice.</p> <h4 id="5-choose-a-solution">5. Choose a Solution</h4> <p>Once I decide on the optimal solution, I confirm with the interviewer before coding. This helps ensure they agree with my approach, avoiding potential misunderstandings.</p> <h4 id="6-code-the-solution">6. Code the Solution</h4> <p>With the go-ahead, I start coding the solution, narrating my thought process to give insight into my choices. For example, I might say, “I’m using a dictionary here to achieve faster lookups, which reduces time complexity.” This part relies heavily on your implementation skills and effective communication.</p> <h4 id="7-dry-run-solution-optional">7. Dry Run Solution (Optional)</h4> <p>If there’s time, I walk through the code with my test cases, effectively “debugging” manually. I step through each line of code, tracking variable changes and outputs to ensure accuracy. For example, here’s how I might track variables:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array = [1, 2, 3, 4]
target = 5
i = 0
</code></pre></div></div> <p>For recursive solutions, I also simulate the call stack, which might look like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>...
fn(array=[1, 2, 3], target=5)
fn(array=[1, 2, 3, 4], target=5)
</code></pre></div></div> <h2 id="reflections-on-time-management">Reflections on Time Management</h2> <p>Managing time well during technical interviews is crucial. Many candidates can solve common patterns with enough time, but the key to standing out is delivering a clear solution within the given timeframe. For optional steps like dry-running, I adjust based on how much time remains. If I’m running low, I may skip the dry run, especially if I didn’t define edge cases earlier. Similarly, if I’m struggling to come up with the optimal solution, I’ll proceed with a simpler approach rather than leaving the problem unsolved.</p> <h2 id="preparation">Preparation</h2> <p>To do well with this formula, I focus on three skills:</p> <ol> <li>Learning algorithm patterns.</li> <li>Improving implementation skills.</li> <li>Practicing clear and interactive communication.</li> </ol> <p>For the first two, practicing problems on platforms like LeetCode is useful, while mock interviews are helpful for improving verbal explanations.</p> <p>I’ve also included some resources that helped me improve these skills:</p> <ul> <li><a href="https://www.designgurus.io/course/grokking-the-coding-interview">Grokking the Coding Interview</a>: Useful for understanding main coding patterns.</li> <li><a href="https://neetcode.io/practice">NeetCode Practice Set</a>: A curated set of practice problems across various patterns.</li> <li><a href="https://leetcode.com/discuss/general-discussion/460599/blind-75-leetcode-questions">Blind 75 LeetCode Questions</a>: A popular list of LeetCode problems that teaches you patterns.</li> </ul> <p>Thanks for reading about my approach to technical interviews. I hope this overview provides a useful perspective, and I’m happy to answer any questions you might have!</p>]]></content><author><name>Ahad Jawaid</name></author><category term="Interview"/><category term="Programming"/><summary type="html"><![CDATA[An overview of how approach technical coding interviews.]]></summary></entry><entry><title type="html">Modeling the Reinforcement Learning Problem</title><link href="https://www.ahadjawaid.com/blog/2023/modeling-reinforcement-learning-problem/" rel="alternate" type="text/html" title="Modeling the Reinforcement Learning Problem"/><published>2023-07-01T00:00:00+00:00</published><updated>2023-07-01T00:00:00+00:00</updated><id>https://www.ahadjawaid.com/blog/2023/modeling-reinforcement-learning-problem</id><content type="html" xml:base="https://www.ahadjawaid.com/blog/2023/modeling-reinforcement-learning-problem/"><![CDATA[<p>This post will discuss how to model the reinforcement learning problem using the mathematical framework of a Markov Decision Process (MDP). We’ll cover what an environment is in reinforcement learning, its components, and how to model it through an MDP.</p> <p>To understand what we are modeling, we must first familiarize ourselves with the components of reinforcement learning:</p> <h2 id="components-of-reinforcement-learning">Components of Reinforcement Learning</h2> <ul> <li><em>Reinforcement Learning</em> is A solution for managing complex, uncertain sequential decision-making. <ul> <li><em>Complex:</em> Pertains to the large scale of the state and action spaces that the agent needs to navigate.</li> <li><em>Sequential:</em> Denotes the delayed effect of an agent’s actions, essentially the credit assignment problem.</li> <li><em>Uncertainty:</em> Highlights the unpredictability of the impact of an agent’s actions on the environment, tying to balance the exploration vs exploitation trade-off.</li> </ul> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/reinforcement-learning-interaction-cycle.jpg" sizes="95vw"/> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/reinforcement-learning-interaction-cycle.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="the-agent-the-decision-maker">The agent: The decision maker</h3> <ul> <li><strong>Agents:</strong> These are entities solely responsible for making decisions that may influence the environment.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/internal-steps.jpg" sizes="95vw"/> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/internal-steps.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li><strong>Agent’s Improvement Process:</strong> <ol> <li>Interact</li> <li>Evaluate</li> <li>Improve</li> </ol> </li> </ul> <h3 id="the-environment-everything-else">The environment: Everything else</h3> <ul> <li>The <em>environment</em> symbolizes the problem at hand. It encapsulates everything external to the agent, anything that’s beyond the agent’s control but responds to the agent’s decisions.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/environment-process.jpg" sizes="95vw"/> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/environment-process.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Pay attention to how we try to emulate the possible reactions of the environment to the agent’s actions.</p> <h2 id="unraveling-the-environment">Unraveling the Environment:</h2> <ul> <li>The environment is made up of states, actions, transition probabilities, and a reward function.</li> </ul> <h3 id="states-specific-configurations-of-the-environment">States: Specific configurations of the environment</h3> <ul> <li> <p>A <em>state</em> provides a comprehensive description of the environment.</p> </li> <li> <p><strong>State Space:</strong> A combiniation of all possible variables and values that can characterize the environment.</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/state-space.jpg" sizes="95vw"/> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/state-space.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p><strong>Initial States:</strong> A subset of states where the agent starts in the environment.</p> </li> <li> <p><strong>Terminal States:</strong> The final state in an episodic task, any transition from this state will leads back to itself.</p> </li> <li> <p><strong>Observation:</strong> The set of variables that the agent perceives. It might not be as comprehensive as a state.</p> </li> <li> <p><strong>Observation Space:</strong> All possible values of the variables observed by the agent.</p> </li> </ul> <h3 id="actions-a-mechanism-to-influence-the-environment">Actions: A mechanism to influence the environment</h3> <ul> <li> <p>An <em>action</em> signifies a choice that an agent can make to potentially alter the environment. The set of actions available to an agent may vary across states and during the agent’s learning process.</p> </li> <li> <p><strong>Action Space:</strong> The set of all actions in all states.</p> </li> </ul> <h3 id="transition-function-consequences-of-agent-actions">Transition Function: Consequences of agent actions</h3> <ul> <li>A <em>transition function</em> determines how the environment changes in response to an action.</li> <li>Denoted as $T(s, a, s’)$, where $s$ is the current state, $a$ is the action taken, and $s’$ is the resulting state.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/transition-function.jpg" sizes="95vw"/> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/transition-function.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p><strong>Stochastic Transitions:</strong> Transitions that don’t guarantee a single resulting state but could lead to multiple states, each with a different probability.</p> </li> <li> <p>In RL, we often assume that transition distributions remain stationary, whether they’re stochastic or deterministic. While this assumption can be relaxed to some degree, for most agents, the transitions must at least appear stationary.</p> </li> </ul> <h3 id="reward-function">Reward Function</h3> <ul> <li> <p>A <em>reward function</em> steers the agent towards its goal. It maps a state or an action to a scalar reward, indicative of the goodness but not necessarily the correctness of the agent’s state or action.</p> </li> <li> <p>Reward signals supervise your agent. A denser reward signal leads to quicker learning but imposes more bias on the agent’s task-solving approach. Conversely, a sparser (less frequent) reward signal results in a less biased agent, potentially leading to novel, emergent behavior.</p> </li> <li> <p><strong>Return:</strong> The sum of the rewards collected in a single episode.</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/reward-function.jpg" sizes="95vw"/> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/reward-function.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="common-types-of-environments">Common Types of Environments:</h3> <ul> <li> <p><strong>Grid-World Environment:</strong> An environment that is a square grid of any size.</p> <ul> <li><strong>Walk / Corridor:</strong> A type of grid-world environment with a single row.</li> </ul> </li> <li> <p><strong>Bandit Environment:</strong> An environment with a single non-terminal state, named ‘bandit’ as an analogy to a slot machine that will empty your pockets the same way bandits do.</p> </li> </ul> <h4 id="ways-to-represent-an-environment">Ways to represent an environment:</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/bandit-walk.jpg" sizes="95vw"/> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/bandit-walk.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/bandit-walk-graph.jpg" sizes="95vw"/> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/bandit-walk-graph.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Table Form</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/bandit-walk-table.jpg" sizes="95vw"/> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/bandit-walk-table.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="markov-decision-process-mdps-the-engine-of-the-environment">Markov Decision Process (MDPs): The engine of the environment</h2> <ul> <li> <p><em>Markov Decision Processes (MDPs)</em> is a mathematical framework for modeling complex decision-making problems under uncertainty. It consists of system states, per-state actions, a transition function, a reward signal, a horizon, a discount factor, and an initial state distribution.</p> </li> <li> <p>In RL, it’s often assumed that all environments operate based on an underlying MDP, whether this assumption holds true or not.</p> </li> </ul> <h3 id="markov-property">Markov Property</h3> <ul> <li>For a state to have the <em>markov property</em>, it must contain all necessary variables to make it independent of all other states.</li> <li>More formally, the probability of the next state, given the current state and action, is independent of the history of interactions.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/markov-property.jpg" sizes="95vw"/> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/markov-property.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>The markov assumption is useful as it allows us to keep the number of variables in a state to a minimum since the more variables you add, the longer it will take the agent to learn. However, completely adhering to the markov assumption may be impractical or even impossible.</li> </ul> <h3 id="horizon-time-changes-whats-optimal">Horizon: Time changes what’s optimal</h3> <ul> <li>A <em>time step</em> is a global clock synchronizing all parties and discretizing time.</li> <li> <p>An <em>episode</em> is a sequence of consecutive time steps from the start to the end of an episodic task.</p> </li> <li><strong>Planning Horizon:</strong> It’s the temporal depth the agent must plan for.</li> <li><strong>Finite Horizon:</strong> It’s a planning horizon that terminates after a specific number of steps.</li> <li><strong>Infite / Indefinite Horizon:</strong> It’s an unlimited planning horizon, assuming that the task can continue indefinitely.</li> <li><strong>Greedy Horizon:</strong> It’s a planning horizon of a single time step.</li> </ul> <h3 id="discount-the-future-is-uncertain-value-it-less">Discount: The future is uncertain, value it less</h3> <ul> <li>To handle the possibility of infinite sequences of interactions, we must convey to the agent that immediate rewards are more valuable than future ones. This concept is handled by introducing a <em>discount factor</em> that reduces the value of future rewards, stabilizing the learning of the task.</li> <li>Moreover, the more we look into the future, the more uncertainty we accumulate. Introducing a discount factor discounts these highly uncertain future rewards, preventing them from affecting our value estimates significantly.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/effect-of-discount-factor.jpg" sizes="95vw"/> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/effect-of-discount-factor.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>The discount factor, $\gamma$ or gamma, reduces the variance of the value prediction by considering future, more uncertain, rewards less than immediate ones, fostering urgency in the agent.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/discount-factor.jpg" sizes="95vw"/> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/discount-factor.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="extensions-to-mdps">Extensions to MDPs</h3> <ul> <li><strong>Partially observable Markov decision process (POMDP):</strong> When the agent can’t fully observe the environment state.</li> <li><strong>Factored Markov decision process (FMDP):</strong> It compactly represents the transition and reward functions, enabling the representation of large MDPs.</li> <li><strong>Continuous Markov decision process:</strong> When either time, action, state, or any combination of them are continuous.</li> <li><strong>Relational Markov decision process (RMDP):</strong> It allows combining probabilistic and relational knowledge.</li> <li><strong>Semi-Markov decision process (SMDP):</strong> It allows the inclusion of abstract actions that take multiple time steps to complete.</li> <li><strong>Multi-agent Markov decision process (MMDP):</strong> It allows multiple agents in the same environment.</li> <li><strong>Decentralized Markov decision process (Dec-MDP):</strong> It allows multiple agents to collaborate and maximize a common reward.</li> </ul> <h3 id="putting-it-all-together">Putting it all together</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/mdps-vs-pomdps.jpg" sizes="95vw"/> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/mdps-vs-pomdps.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="references">References</h2> <p>Morales, M. (2020). Grokking Deep Reinforcement Learning. Originally Published: October 15, 2020.</p> <p><em>All figures are sourced from this book.</em></p>]]></content><author><name>Ahad Jawaid</name></author><category term="Deep Reinforcement Learning"/><summary type="html"><![CDATA[My notes on the second chapter of 'Grokking Deep Reinforcement Learning' by Miguel Morales. This post covers the components of the environment and how to model it using Markov Decision Processes (MDPs).]]></summary></entry><entry><title type="html">Introduction to Deep Reinforcement Learning</title><link href="https://www.ahadjawaid.com/blog/2023/introduction-to-deep-reinforcement-learning/" rel="alternate" type="text/html" title="Introduction to Deep Reinforcement Learning"/><published>2023-06-27T00:00:00+00:00</published><updated>2023-06-27T00:00:00+00:00</updated><id>https://www.ahadjawaid.com/blog/2023/introduction-to-deep-reinforcement-learning</id><content type="html" xml:base="https://www.ahadjawaid.com/blog/2023/introduction-to-deep-reinforcement-learning/"><![CDATA[<style>.responsive-img{width:100%}@media(min-width:992px){.responsive-img{width:75%}}</style> <h2 id="artificial-intelligence-ai">Artificial Intelligence (AI)</h2> <ul> <li>Artificial Intelligence (AI) is a domain of computer science dedicated to developing software capable of exhibiting attributes of intelligence.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/subfields_of_artificial_intelligence.jpg" sizes="95vw"/> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/subfields_of_artificial_intelligence.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="machine-learning-ml">Machine Learning (ML)</h2> <ul> <li>Machine learning, a subset of AI, tackles problems necessitating intelligent solutions by learning from data.</li> <li><strong>Supervised Learning (SL):</strong> A method that learns from labeled data. <ul> <li>E.g., handwritten-digit-recognition</li> </ul> </li> <li><strong>Unsupervised Learning (UL):</strong> A method that learns from unlabeled data <ul> <li>E.g., customer segmentaiton</li> </ul> </li> <li><strong>Reinforcement Learning (RL):</strong> A method that learns from trial and error <ul> <li>E.g., pong-playing agent</li> </ul> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/main_branches_of_machine_learning.jpg" sizes="95vw"/> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/main_branches_of_machine_learning.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="deep-learning-dl">Deep Learning (DL)</h2> <ul> <li>Deep Learning employs multi-layered non-linear function approximations, also known as neural networks, to address ML tasks. Essentially, it is a suite of techniques that utilize neural networks to solve ML challenges.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/deep_learning_is_a_powerful_toolbox.jpg" sizes="95vw"/> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/deep_learning_is_a_powerful_toolbox.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="deep-reinforcement-learning-drl">Deep Reinforcement Learning (DRL)</h3> <ul> <li>Deep Reinforcement Learning learns through trial and error from feedback that’s simultaneously sequentially, evaluative, and sampled by leveraging non-linear function approximation (neural networks).</li> </ul> <h2 id="reinforcement-learning-rl">Reinforcement Learning (RL)</h2> <h3 id="similar-fields">Similar fields</h3> <ul> <li><strong>Reinforcement Learning (RL):</strong> Investigates methods of resolving complex sequential decision-making problems under uncertain conditions.</li> <li><strong>Control Theory (CT)</strong>: Examines methods of controlling complex known dynamic systems.</li> <li><strong>Operations Research (OR)</strong>: Investigates decision-making under uncertain conditions, generally featuring a larger action space than in DRL.</li> <li><strong>Psychology</strong>: Studies human behavior, which frequently encapsulates complex sequential decision-making problems under uncertainty.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/the_synergy_between_similar_fields.jpg" sizes="95vw"/> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/the_synergy_between_similar_fields.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="agent-and-enviroment">Agent and Enviroment</h3> <ul> <li> <p><strong>Agent:</strong> Refers exclusively to the decision-making entity.</p> <ul> <li>For instance, if you are training a robot arm to pick up a toy, the agent is the code that makes the decisions, not the robot arm itself.</li> </ul> </li> <li> <p><strong>Environment:</strong> Includes everything external to the agent, beyond the agent’s control, and everything that follows the agent’s decisions.</p> <ul> <li>In the context of training a robot arm to pick up a toy, the objects to be picked up, the tray where the objects reside, atmospheric conditions like wind, and even the robot arm itself are all considered parts of the environment.</li> </ul> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/boundary_between_agent_and_environment.jpg" sizes="95vw"/> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/boundary_between_agent_and_environment.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="states-and-observations">States and Observations</h3> <ul> <li><strong>State Space:</strong> The set of all possible variables and values that can represent the state of the environment.</li> <li><strong>State:</strong> A comprehensive description of the environment, or an instantiation of the state space.</li> <li><strong>Observation:</strong> A partial or incomplete description of the environment.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/states_vs_observations.jpg" sizes="95vw"/> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/states_vs_observations.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="reinforcement-learning-cycle">Reinforcement Learning Cycle</h3> <ul> <li><strong>Transition Function:</strong> The mapping from the agent’s action to a potentially new state.</li> <li><strong>Reward Function:</strong> The mapping from the action taken to the potential reward signal. <ul> <li>Goals are defined via the reward function.</li> </ul> </li> <li><strong>Model:</strong> A set of the transitions and rewards.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/the_reinforcement_learning_cycle.jpg" sizes="95vw"/> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/the_reinforcement_learning_cycle.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h4 id="agents-improvement-process">Agent’s Improvement process:</h4> <ol> <li>Interact with the environment.</li> <li>Evaluates its behavior.</li> <li>Improves its responses.</li> </ol> <h4 id="agents-can-be-designed-to-learn">Agent’s can be designed to learn:</h4> <ul> <li><strong>Policy:</strong> The mapping from observations to actions.</li> <li><strong>Models:</strong> A model of the environment on mappings.</li> <li><strong>Value Functions:</strong> The mapping of a state to its estimated value.</li> </ul> <h3 id="experiences">Experiences</h3> <ul> <li><strong>Time Step:</strong> A single cycle of interaction between the agent and the environment.</li> <li><strong>Experience:</strong> The set consisting of the state, the action, the reward, and the new state in a single time step.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/experience_tuples.jpg" sizes="95vw"/> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/experience_tuples.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li><strong>Episodic Task:</strong> Tasks that have a natural ending or goes on finitely many step. <ul> <li>E.g., video games</li> </ul> </li> <li><strong>Continuing Task:</strong> Tasks that don’t have a natural ending or could go on indefinitely. <ul> <li>E.g., learning forward motion</li> </ul> </li> </ul> <h3 id="credit-assignment-problem">Credit Assignment Problem</h3> <ul> <li><strong>Temporal Credit Assignment Problem:</strong> the challenge in determining which state and/or action is responsible for a reward the agent recieves <ul> <li>Usually occurs when the agent may have delayed rewards from an action or state that caused it hence the temporal aspect of the problem</li> </ul> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/temporal_credit_assignment_problem.jpg" sizes="95vw"/> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/temporal_credit_assignment_problem.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="exploration-vs-exploitation">Exploration vs. Exploitation</h3> <ul> <li><strong>Evaluative Feedback:</strong> Feedback that provides an indication of performance but not correctness.</li> <li><strong>Exploration versus Exploitation trade-off:</strong> The balance between collecting new information from the environment and using known information to maximize rewards.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/exploration_vs_exploitation.jpg" sizes="95vw"/> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/exploration_vs_exploitation.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="sampled-feedback">Sampled Feedback</h3> <ul> <li>Learning from sparse or weak feedback becomes more challenging with samples only. The agent must be capable of generalizing to learn from sampled feedback.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/learning_from_sampled_feedback.jpg" sizes="95vw"/> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/learning_from_sampled_feedback.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="types-of-agents">Types of Agents</h3> <ul> <li><strong>Policy-based:</strong> Designed to approximate policies.</li> <li><strong>Model-based:</strong> Designed to approximate models.</li> <li><strong>Value-based:</strong> Designed to approximate value functions.</li> <li><strong>Actor-critic:</strong> Designed to approximate both policies and value functions.</li> </ul> <h3 id="pros-and-cons">Pros and Cons</h3> <p><strong>Strength:</strong> Reinforcement learning excels in mastering specific tasks.</p> <p><strong>Weaknesses:</strong> To learn a well-performing policy, it generally requires millions of samples.</p> <h2 id="history-of-deep-reinforcement-learning">History of Deep Reinforcement Learning</h2> <ul> <li> <p><strong>Alan Turing - 1930:</strong> Developed the Turing Test, a test of a machine’s ability to exhibit intelligent behavior indistinguishable from that of a human.</p> </li> <li> <p><strong>John McCarthy - 1955:</strong> Coined the term “Artificial Intelligence”.</p> </li> <li> <p><strong>Andrew Ng - 2002:</strong> Trained an autonomous helicopter to perform stunts by observing human-expert flights using inverse reinforcement learning.</p> </li> <li> <p><strong>Nate Kohl and Peter Stone - 2002:</strong> Applied policy-gradient methods to train a soccer-playing robot.</p> </li> <li> <p><strong>Mnih et al. - 2013, 2015:</strong> Introduced the DQN algorithm, which learned to play Atari games from raw pixels.</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/atari_dqn.jpg" sizes="95vw"/> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/atari_dqn.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>*</p> <ul> <li> <p><strong>Silver et al. - 2014:</strong> Released the deterministic policy gradient (DPG) algorithm.</p> </li> <li> <p><strong>Lillicrap et al. - 2015:</strong> Improved DPG with deep deterministic policy gradient (DDPG)</p> </li> <li> <p><strong>Schulman et al. - 2016:</strong> Released trust region policy optimization (TRPO) and generalized advantage estimation (GAE) methods.</p> </li> <li> <p><strong>Sergey Levine et al. - 2016:</strong> Published Guided Policy Search (GPS)</p> </li> <li> <p><strong>Silver et al. - 2016:</strong> Demonstrated AlphaGo</p> </li> <li> <p>…</p> </li> </ul> <h2 id="references">References</h2> <p>Morales, M. (2020). Grokking Deep Reinforcement Learning. Originally Published: October 15, 2020.</p> <p><em>All figures are sourced from this book.</em></p>]]></content><author><name>Ahad Jawaid</name></author><category term="Deep Reinforcement Learning"/><category term="Notes"/><summary type="html"><![CDATA[My notes on Deep Reinforcement Learning (DRL) based on the first chapter of the 'Grokking Deep Reinforcement Learning' by Miguel Morales.]]></summary></entry><entry><title type="html">From Paper to Code: A Guide to Implement Research</title><link href="https://www.ahadjawaid.com/blog/2023/translating-theory-into-code/" rel="alternate" type="text/html" title="From Paper to Code: A Guide to Implement Research"/><published>2023-06-05T00:00:00+00:00</published><updated>2023-06-05T00:00:00+00:00</updated><id>https://www.ahadjawaid.com/blog/2023/translating-theory-into-code</id><content type="html" xml:base="https://www.ahadjawaid.com/blog/2023/translating-theory-into-code/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Recently, I’ve been diving into the practice of implementing deep learning research papers. This post is designed to guide you through this process, sharing my key insights along the way.</p> <p>Starting with an independent study course at university, I explored deep learning intensively. To streamline the journey of understanding and implementing such papers, I’ve distilled the process into four steps:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/translating-theory-into-code/read_code_visualize_diagram.svg" sizes="95vw"/> <img src="/assets/img/posts/translating-theory-into-code/read_code_visualize_diagram.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Let’s break each one down.</p> <h2 id="read">Read</h2> <p>Reading a research paper should be focused on understanding the process rather than memorizing details. Pay attention to the inputs, outputs, data transformations, and assumptions.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/translating-theory-into-code/questions.svg" sizes="95vw"/> <img src="/assets/img/posts/translating-theory-into-code/questions.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To expedite the reading process, it helps to know where to find necessary details. Typically, the most crucial sections in a research paper are:</p> <ol> <li> <p><strong>Abstract</strong></p> <ul> <li>Provides a broad understanding of the paper and its relevance.</li> </ul> </li> <li> <p><strong>Figures</strong></p> <ul> <li>Often encapsulate the paper’s content and are valuable for understanding the model’s workings. However, ensure their accuracy by cross-referencing the textual content.</li> </ul> </li> <li> <p><strong>Introduction</strong></p> <ul> <li>Highlights the paper’s novelty or significance.</li> </ul> </li> <li> <p><strong>Architecture (Can go by other names)</strong></p> <ul> <li>Contains detailed descriptions of the architecture (usually where most of your time will be spent).</li> </ul> </li> <li> <p><strong>Training</strong></p> <ul> <li>Provides hyperparameters for training, model performance, and other essential information.</li> </ul> </li> <li> <p><strong>The appendix</strong></p> <ul> <li>Often contains implementation details not fitting into the main paper’s flow.</li> </ul> </li> <li> <p><strong>Experiements</strong> (Optional)</p> <ul> <li>Can include important data and parameters used for model training.</li> </ul> </li> </ol> <h2 id="implement">Implement</h2> <p>During implementation, the key is not to get overwhelmed by the complexity of the task. Instead, focus on the components you understand, and incrementally build upon them. It’s okay if your code isn’t flawless initially; revisions can be made as you progress.</p> <h2 id="visualize">Visualize</h2> <p>Post-implementation, validate your code. Use libraries like <a href="https://matplotlib.org/">matplotlib</a> to visualize the outputs, or tools like <a href="https://docs.pytest.org/en/7.3.x/">pytest</a> to perform tests. Working in an interactive environment like a <a href="https://jupyter.org/">jupyter notebook</a> can also be helpful.</p> <h2 id="repeat">Repeat</h2> <p>The key to successful implementation is repetition. Even if you start slowly, your pace will increase as you build your knowledge and develop better abstractions. This process also improves your programming skills.</p> <h2 id="conclusion">Conclusion</h2> <p>In summary, the methodology I formulated and the key lessons I learned during my initial experience implementing a research paper can be encapsulated in four words: read, implement, visualize, and repeat. I hope you find this post beneficial, and I welcome any feedback or suggestions you might have. Thank you!</p>]]></content><author><name>Ahad Jawaid</name></author><category term="Research"/><summary type="html"><![CDATA[This guide covers effective reading, model implementation, code validation, and the power of repetition.]]></summary></entry><entry><title type="html">Neural network: The universal function</title><link href="https://www.ahadjawaid.com/blog/2023/the-universal-function/" rel="alternate" type="text/html" title="Neural network: The universal function"/><published>2023-02-23T00:00:00+00:00</published><updated>2023-02-23T00:00:00+00:00</updated><id>https://www.ahadjawaid.com/blog/2023/the-universal-function</id><content type="html" xml:base="https://www.ahadjawaid.com/blog/2023/the-universal-function/"><![CDATA[<p>Have you ever imagined a single machine that can adapt to any role given to it? It could be a screw, a tire, a screen or any other task you could think of. That’s what a neural network is for software — a function that can perform any task.</p> <p>Initially, when I learned about neural networks, it seemed like a highly abstract concept that emulated the brain. However, after reading books such as Deep Learning by Ian Goodfellow and The Deep Learning practical coder by Jeremy Howard, I came to realize that a neural network is nothing more than a function that can be adjusted to perform any task we require of it.</p> <h2 id="what-exactly-is-a-function">What exactly is a function?</h2> <p>A function is a mapping of an input to an output. For instance, an addition function could take two numbers as inputs and output their sum.</p> <h2 id="adjustable-function">Adjustable function</h2> <p>An adjustable function, on the other hand, is a function that has weights or parameters that regulate how the inputs are modified to generate a specific output. For example, a function f that takes two numbers as inputs can be expressed as</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">f(input1, input2) = weight1 * input1 + weight2 * input2</code></p> </blockquote> <p>Here, we can change the function by adjusting the weights.</p> <h2 id="how-can-an-adjustable-function-learn-a-task">How can an adjustable function learn a task?</h2> <p>Now that we have established what an adjustable function is, let’s look at how we can use it to solve a problem. The process of adjusting the weights in the function to accomplish a task involves three main steps:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. Get output from the function
2. Check how wrong the output is compared to 3. the desired output
4. Adjust the weights to make the output look more like the desired output
</code></pre></div></div> <p>This process can be illustrated in the following diagram:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/the-universal-function/model.png" sizes="95vw"/> <img src="/assets/img/posts/the-universal-function/model.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>As simple as this may seem, this is the core of how a neural network learns a task. It starts by guessing an output, and then we adjust the weights to make it look more like the desired output.</p> <h2 id="adjusting-the-weights-automatically">Adjusting the weights automatically</h2> <p>Adjusting the weights automatically is where it gets interesting. Let’s assume you’re playing a game where someone has a number in mind, and you have to guess it. Every time you make a guess, they tell you whether you’re close or far. If you randomly pick the numbers at random, you’d get nowhere. But adding two simple steps to the process can solve this problem:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. A method to determine how far you are from the desired output (output error method)
2.A method for finding the direction and extent of the weights to be changed (method for determining how to adjust weights)
</code></pre></div></div> <h2 id="output-error-method">Output Error Method</h2> <p>To find how far we are from the desired output, we can subtract our function’s output and the desired output,</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">f(input1, input2) = 5</code> &gt; <code class="language-plaintext highlighter-rouge">goal_function(input1, input2) = 10</code> &gt; <code class="language-plaintext highlighter-rouge">error = 5–10 = -5</code></p> </blockquote> <p>The problem with this method is that it could give us negative values, which can be problematic when attempting to minimize the error. To address this issue, we can bound the method by taking the absolute value of the subtraction (positive number).</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">error = | 5–10 | = 5</code></p> </blockquote> <h2 id="method-for-determining-how-to-adjust-weights">Method for determining how to adjust weights</h2> <p>The last addition to adjust the weights automatically is to find the direction and extent of the weights to be changed. We can accomplish this by using a concept from calculus known as the derivative, which is the slope of the function at a specific point. By finding the slope or derivative, we can determine the direction and amount to change the weights.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/the-universal-function/tanget.png" sizes="95vw"/> <img src="/assets/img/posts/the-universal-function/tanget.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>So in this illustration, we can find that the slope at point one is two by using the rise-over run of the tangent line (a line that touches the function at a point). We can use this slope or derivative to determine in what direction and how much to change the weights. This is done because of a property that a derivative has, which is when the derivative is zero, it is at the minimum or maximum (or saddle point) of the function. We can take advantage of this property with our method of measuring the error to find the minimum of the error.</p> <p>So if we keep adjusting the weights and the derivative gets smaller, we are approaching a minimum or maximum (or saddle point). To ensure we are finding the minimum and not the maximum error, we need to figure out what direction we should change the weights. This can be done simply by subtracting the derivative to act as we descend to the minimum. Putting it together</p> <p>The heart of deep learning lies in the ability of neural networks to learn any task by going through five steps. These are:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. Input data into the function
2. Compare the output to the error function (loss function)
3. Take the derivative of the error function with respect to the weights
4. Subtract the derivative of the weights from the weights
5. Repeat until the error is small
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/the-universal-function/universal-function.png" sizes="95vw"/> <img src="/assets/img/posts/the-universal-function/universal-function.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Everything else is focused on making the training process efficient and timely.</p> <h2 id="the-universal-function">The Universal Function</h2> <p>In conclusion, the neural network is a powerful tool that allows software to adapt to any role given to them. At first glance, it may seem like an abstract concept, but it is nothing more guessing and checking then improving. What’s fascinating about this is that the improvement can be made automatically by using concepts from calculus like the derivative, which allows us to determine how to improve. With these tools, the neural network can learn any task and solve most problem thrown its way.</p> <p>Today, we see the applications of neural networks in various fields, including speech recognition, image and pattern recognition, and natural language processing, to name a few. The possibilities of what we can achieve with this technology are endless, and we are only scratching the surface of what it can do.</p>]]></content><author><name>Ahad Jawaid</name></author><category term="Deep Learning"/><category term="Neueral Networks"/><summary type="html"><![CDATA[Have you ever imagined a single machine that can adapt to any role given to it? It could be a screw, a tire, a screen or any other task you could think of. That’s what a neural network is for software — a function that can perform any task.]]></summary></entry></feed>