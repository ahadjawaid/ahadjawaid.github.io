<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://www.ahadjawaid.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://www.ahadjawaid.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-03T13:18:51+00:00</updated><id>https://www.ahadjawaid.com/feed.xml</id><title type="html">Ahad Jawaid</title><subtitle>Hi! I&apos;m Ahad, and I&apos;m a software developer and a researcher who likes to build cool things :)</subtitle><entry><title type="html"></title><link href="https://www.ahadjawaid.com/blog/2024/2024-03-11-nsf-grfp/" rel="alternate" type="text/html" title=""/><published>2024-11-03T13:18:51+00:00</published><updated>2024-11-03T13:18:51+00:00</updated><id>https://www.ahadjawaid.com/blog/2024/2024-03-11-nsf-grfp</id><content type="html" xml:base="https://www.ahadjawaid.com/blog/2024/2024-03-11-nsf-grfp/"><![CDATA[<p>The National Science Foundation (NSF) awards $159,000 to up to 2,300 Graduate Research Fellowships (GRFP) every year. In 2024, the NSF awarded fellowships to 14.44% of applicants [1]. This program is intended for U.S. citizens.</p> <p>I recently completed my application for the NSF GRFP and wanted to share the stuff I found important from the process, from understanding the requirements to crafting a research proposal. I applied under the Computer Science - Robotics topic for the 2025 cycle.</p> <p>This post covers:</p> <ul> <li>Statistics for the Program</li> <li>NSF Fellowship Solicitation</li> <li>Review Rubric</li> <li>Discussions</li> </ul> <p>This post is part of a mini blog series, including:</p> <ol> <li>Applying to NSF Graduate Fellowship 2024 (This post)</li> <li>Writing a Research Proposal</li> <li>Writing a Personal Statement</li> <li>Asking for Letters of Recommendation</li> </ol> <h3 id="statistics-for-the-program">Statistics for the Program</h3> <p>After applying, I analyzed the 2024 awardee statistics. Here’s a breakdown:</p> <p><strong>Distribution of recipient majors:</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/nsf-grfp/nsf_grfp_major_distribution.png" sizes="95vw"/> <img src="/assets/img/posts/nsf-grfp/nsf_grfp_major_distribution.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Top 35 fields accepted in the NSF GRFP:</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/nsf-grfp/nsf-grfp-top-35-subfields.png" sizes="95vw"/> <img src="/assets/img/posts/nsf-grfp/nsf-grfp-top-35-subfields.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Top computer science subfields accepted:</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/nsf-grfp/nsf-grfp-cs-subfields.png" sizes="95vw"/> <img src="/assets/img/posts/nsf-grfp/nsf-grfp-cs-subfields.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Top 20 institutions by number of recipients:</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/nsf-grfp/nsf-grfp-top-20-schools.png" sizes="95vw"/> <img src="/assets/img/posts/nsf-grfp/nsf-grfp-top-20-schools.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>These statistics were derived from public NSF data, accessible <a href="https://www.research.gov/grfp/AwardeeList.do?method=loadAwardeeList">here</a> [2].</p> <h2 id="nsf-fellowship-solicitation">NSF Fellowship Solicitation</h2> <p>A crucial resource for applying is the program’s <a href="https://new.nsf.gov/funding/opportunities/grfp-nsf-graduate-research-fellowship-program/nsf24-591/solicitation">solicitation</a> [3]. It outlines all essential information about the program, application guidelines, and goals.</p> <h3 id="program-goals">Program Goals</h3> <p>According to the solicitation [3], the goals of the program are to:</p> <ol> <li>Select individuals with the <strong>demonstrated potential</strong> to be <strong>high-achieving</strong> scientists and engineers.</li> <li><strong>Broaden participation</strong> of the full spectrum of diverse talents in STEM.</li> </ol> <h3 id="application-components">Application Components</h3> <p>The application includes:</p> <ul> <li>Reference Letters</li> <li>Personal Statement</li> <li>Research Statement</li> </ul> <p>Ensure you follow all formatting guidelines, as <strong>failure</strong> to do so can lead to <strong>automatic rejection</strong>.</p> <p>Each statement should address:</p> <ul> <li><strong>Intellectual Merit:</strong> This section should highlight your potential as a scientist or engineer, including research experience, academic achievements, and industry experience. It usually has more detail than the broader impacts section.</li> <li><strong>Broader Impacts:</strong> In the research statement, broader impacts should relate to your research. In the personal statement, they should focus on activities outside of your research. Address the societal benefits of your work, such as: <ul> <li>Full participation of women, persons with disabilities, and underrepresented minorities in STEM.</li> <li>Improved STEM education and educator development at any level.</li> <li>Increased public scientific literacy and engagement with science and technology.</li> <li>Improved well-being of individuals in society.</li> <li>Development of a diverse, globally competitive STEM workforce.</li> <li>Increased partnerships between academia, industry, and other sectors.</li> <li>Improved national security.</li> <li>Increased economic competitiveness of the United States.</li> <li>Enhanced infrastructure for research and education.</li> </ul> </li> </ul> <p>Reference letters should also discuss both intellectual merit and broader impacts.</p> <h2 id="review-rubric">Review Rubric</h2> <p>The review rubric for the NSF GRFP includes:</p> <p><strong>Intellectual Merit:</strong><br/> Rating from 1 (poorly discussed) to 7 (excellent):</p> <ul> <li>Does the proposed activity advance knowledge within its field or across fields?</li> <li>Are the concepts creative, original, or potentially transformative?</li> <li>Is the plan well-reasoned, organized, and based on a sound rationale?</li> </ul> <p><strong>Broader Impact:</strong></p> <ul> <li>Will the activity benefit society or achieve desirable societal outcomes?</li> <li>How well do societal outcomes relate to the proposed activity?</li> </ul> <p><strong>Other Considerations:</strong></p> <ul> <li>Are the activities feasible within the time frame?</li> </ul> <p>The rubric comes from the solicitation [3], while the scoring system was explained during an NSF GRFP bootcamp at my university.</p> <h2 id="discussions">Discussions</h2> <p>Based on past applications found at <a href="https://www.alexhunterlang.com/nsf-fellowship">Alex Hunter Lang Website</a> [4], successful applicants often have at least one research publication.</p> <p>Writing my NSF GRFP application took about five months, juggling classes and research. I recommend starting early and refining your statements with feedback from peers. Clarity and reliability are critical.</p> <p>Feel free to reach out if you have questions via <a href="https://twitter.com/ahadjawaid">Twitter</a>, <a href="https://www.linkedin.com/in/ahadjawaid/">LinkedIn</a>, or email.</p> <h1 id="references">References</h1> <p>[1] https://www.nsf.gov/edu/GRFPFacts.jsp<br/> [2] https://www.research.gov/grfp/AwardeeList.do?method=loadAwardeeList<br/> [3] https://new.nsf.gov/funding/opportunities/grfp-nsf-graduate-research-fellowship-program/nsf24-591/solicitation<br/> [4] https://www.alexhunterlang.com/nsf-fellowship</p>]]></content><author><name></name></author></entry><entry><title type="html">Modeling the Reinforcement Learning Problem</title><link href="https://www.ahadjawaid.com/blog/2023/modeling-reinforcement-learning-problem/" rel="alternate" type="text/html" title="Modeling the Reinforcement Learning Problem"/><published>2023-07-01T00:00:00+00:00</published><updated>2023-07-01T00:00:00+00:00</updated><id>https://www.ahadjawaid.com/blog/2023/modeling-reinforcement-learning-problem</id><content type="html" xml:base="https://www.ahadjawaid.com/blog/2023/modeling-reinforcement-learning-problem/"><![CDATA[<p>This post will discuss how to model the reinforcement learning problem using the mathematical framework of a Markov Decision Process (MDP). We’ll cover what an environment is in reinforcement learning, its components, and how to model it through an MDP.</p> <p>To understand what we are modeling, we must first familiarize ourselves with the components of reinforcement learning:</p> <h2 id="components-of-reinforcement-learning">Components of Reinforcement Learning</h2> <ul> <li><em>Reinforcement Learning</em> is A solution for managing complex, uncertain sequential decision-making. <ul> <li><em>Complex:</em> Pertains to the large scale of the state and action spaces that the agent needs to navigate.</li> <li><em>Sequential:</em> Denotes the delayed effect of an agent’s actions, essentially the credit assignment problem.</li> <li><em>Uncertainty:</em> Highlights the unpredictability of the impact of an agent’s actions on the environment, tying to balance the exploration vs exploitation trade-off.</li> </ul> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/reinforcement-learning-interaction-cycle.jpg" sizes="95vw"/> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/reinforcement-learning-interaction-cycle.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="the-agent-the-decision-maker">The agent: The decision maker</h3> <ul> <li><strong>Agents:</strong> These are entities solely responsible for making decisions that may influence the environment.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/internal-steps.jpg" sizes="95vw"/> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/internal-steps.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li><strong>Agent’s Improvement Process:</strong> <ol> <li>Interact</li> <li>Evaluate</li> <li>Improve</li> </ol> </li> </ul> <h3 id="the-environment-everything-else">The environment: Everything else</h3> <ul> <li>The <em>environment</em> symbolizes the problem at hand. It encapsulates everything external to the agent, anything that’s beyond the agent’s control but responds to the agent’s decisions.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/environment-process.jpg" sizes="95vw"/> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/environment-process.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Pay attention to how we try to emulate the possible reactions of the environment to the agent’s actions.</p> <h2 id="unraveling-the-environment">Unraveling the Environment:</h2> <ul> <li>The environment is made up of states, actions, transition probabilities, and a reward function.</li> </ul> <h3 id="states-specific-configurations-of-the-environment">States: Specific configurations of the environment</h3> <ul> <li> <p>A <em>state</em> provides a comprehensive description of the environment.</p> </li> <li> <p><strong>State Space:</strong> A combiniation of all possible variables and values that can characterize the environment.</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/state-space.jpg" sizes="95vw"/> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/state-space.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p><strong>Initial States:</strong> A subset of states where the agent starts in the environment.</p> </li> <li> <p><strong>Terminal States:</strong> The final state in an episodic task, any transition from this state will leads back to itself.</p> </li> <li> <p><strong>Observation:</strong> The set of variables that the agent perceives. It might not be as comprehensive as a state.</p> </li> <li> <p><strong>Observation Space:</strong> All possible values of the variables observed by the agent.</p> </li> </ul> <h3 id="actions-a-mechanism-to-influence-the-environment">Actions: A mechanism to influence the environment</h3> <ul> <li> <p>An <em>action</em> signifies a choice that an agent can make to potentially alter the environment. The set of actions available to an agent may vary across states and during the agent’s learning process.</p> </li> <li> <p><strong>Action Space:</strong> The set of all actions in all states.</p> </li> </ul> <h3 id="transition-function-consequences-of-agent-actions">Transition Function: Consequences of agent actions</h3> <ul> <li>A <em>transition function</em> determines how the environment changes in response to an action.</li> <li>Denoted as $T(s, a, s’)$, where $s$ is the current state, $a$ is the action taken, and $s’$ is the resulting state.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/transition-function.jpg" sizes="95vw"/> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/transition-function.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p><strong>Stochastic Transitions:</strong> Transitions that don’t guarantee a single resulting state but could lead to multiple states, each with a different probability.</p> </li> <li> <p>In RL, we often assume that transition distributions remain stationary, whether they’re stochastic or deterministic. While this assumption can be relaxed to some degree, for most agents, the transitions must at least appear stationary.</p> </li> </ul> <h3 id="reward-function">Reward Function</h3> <ul> <li> <p>A <em>reward function</em> steers the agent towards its goal. It maps a state or an action to a scalar reward, indicative of the goodness but not necessarily the correctness of the agent’s state or action.</p> </li> <li> <p>Reward signals supervise your agent. A denser reward signal leads to quicker learning but imposes more bias on the agent’s task-solving approach. Conversely, a sparser (less frequent) reward signal results in a less biased agent, potentially leading to novel, emergent behavior.</p> </li> <li> <p><strong>Return:</strong> The sum of the rewards collected in a single episode.</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/reward-function.jpg" sizes="95vw"/> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/reward-function.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="common-types-of-environments">Common Types of Environments:</h3> <ul> <li> <p><strong>Grid-World Environment:</strong> An environment that is a square grid of any size.</p> <ul> <li><strong>Walk / Corridor:</strong> A type of grid-world environment with a single row.</li> </ul> </li> <li> <p><strong>Bandit Environment:</strong> An environment with a single non-terminal state, named ‘bandit’ as an analogy to a slot machine that will empty your pockets the same way bandits do.</p> </li> </ul> <h4 id="ways-to-represent-an-environment">Ways to represent an environment:</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/bandit-walk.jpg" sizes="95vw"/> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/bandit-walk.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/bandit-walk-graph.jpg" sizes="95vw"/> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/bandit-walk-graph.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Table Form</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/bandit-walk-table.jpg" sizes="95vw"/> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/bandit-walk-table.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="markov-decision-process-mdps-the-engine-of-the-environment">Markov Decision Process (MDPs): The engine of the environment</h2> <ul> <li> <p><em>Markov Decision Processes (MDPs)</em> is a mathematical framework for modeling complex decision-making problems under uncertainty. It consists of system states, per-state actions, a transition function, a reward signal, a horizon, a discount factor, and an initial state distribution.</p> </li> <li> <p>In RL, it’s often assumed that all environments operate based on an underlying MDP, whether this assumption holds true or not.</p> </li> </ul> <h3 id="markov-property">Markov Property</h3> <ul> <li>For a state to have the <em>markov property</em>, it must contain all necessary variables to make it independent of all other states.</li> <li>More formally, the probability of the next state, given the current state and action, is independent of the history of interactions.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/markov-property.jpg" sizes="95vw"/> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/markov-property.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>The markov assumption is useful as it allows us to keep the number of variables in a state to a minimum since the more variables you add, the longer it will take the agent to learn. However, completely adhering to the markov assumption may be impractical or even impossible.</li> </ul> <h3 id="horizon-time-changes-whats-optimal">Horizon: Time changes what’s optimal</h3> <ul> <li>A <em>time step</em> is a global clock synchronizing all parties and discretizing time.</li> <li> <p>An <em>episode</em> is a sequence of consecutive time steps from the start to the end of an episodic task.</p> </li> <li><strong>Planning Horizon:</strong> It’s the temporal depth the agent must plan for.</li> <li><strong>Finite Horizon:</strong> It’s a planning horizon that terminates after a specific number of steps.</li> <li><strong>Infite / Indefinite Horizon:</strong> It’s an unlimited planning horizon, assuming that the task can continue indefinitely.</li> <li><strong>Greedy Horizon:</strong> It’s a planning horizon of a single time step.</li> </ul> <h3 id="discount-the-future-is-uncertain-value-it-less">Discount: The future is uncertain, value it less</h3> <ul> <li>To handle the possibility of infinite sequences of interactions, we must convey to the agent that immediate rewards are more valuable than future ones. This concept is handled by introducing a <em>discount factor</em> that reduces the value of future rewards, stabilizing the learning of the task.</li> <li>Moreover, the more we look into the future, the more uncertainty we accumulate. Introducing a discount factor discounts these highly uncertain future rewards, preventing them from affecting our value estimates significantly.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/effect-of-discount-factor.jpg" sizes="95vw"/> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/effect-of-discount-factor.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>The discount factor, $\gamma$ or gamma, reduces the variance of the value prediction by considering future, more uncertain, rewards less than immediate ones, fostering urgency in the agent.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/discount-factor.jpg" sizes="95vw"/> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/discount-factor.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="extensions-to-mdps">Extensions to MDPs</h3> <ul> <li><strong>Partially observable Markov decision process (POMDP):</strong> When the agent can’t fully observe the environment state.</li> <li><strong>Factored Markov decision process (FMDP):</strong> It compactly represents the transition and reward functions, enabling the representation of large MDPs.</li> <li><strong>Continuous Markov decision process:</strong> When either time, action, state, or any combination of them are continuous.</li> <li><strong>Relational Markov decision process (RMDP):</strong> It allows combining probabilistic and relational knowledge.</li> <li><strong>Semi-Markov decision process (SMDP):</strong> It allows the inclusion of abstract actions that take multiple time steps to complete.</li> <li><strong>Multi-agent Markov decision process (MMDP):</strong> It allows multiple agents in the same environment.</li> <li><strong>Decentralized Markov decision process (Dec-MDP):</strong> It allows multiple agents to collaborate and maximize a common reward.</li> </ul> <h3 id="putting-it-all-together">Putting it all together</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/modeling-reinforcement-learning-problem/mdps-vs-pomdps.jpg" sizes="95vw"/> <img src="/assets/img/posts/modeling-reinforcement-learning-problem/mdps-vs-pomdps.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="references">References</h2> <p>Morales, M. (2020). Grokking Deep Reinforcement Learning. Originally Published: October 15, 2020.</p> <p><em>All figures are sourced from this book.</em></p>]]></content><author><name>Ahad Jawaid</name></author><category term="Deep Reinforcement Learning"/><summary type="html"><![CDATA[My notes on the second chapter of 'Grokking Deep Reinforcement Learning' by Miguel Morales. This post covers the components of the environment and how to model it using Markov Decision Processes (MDPs).]]></summary></entry><entry><title type="html">Introduction to Deep Reinforcement Learning</title><link href="https://www.ahadjawaid.com/blog/2023/introduction-to-deep-reinforcement-learning/" rel="alternate" type="text/html" title="Introduction to Deep Reinforcement Learning"/><published>2023-06-27T00:00:00+00:00</published><updated>2023-06-27T00:00:00+00:00</updated><id>https://www.ahadjawaid.com/blog/2023/introduction-to-deep-reinforcement-learning</id><content type="html" xml:base="https://www.ahadjawaid.com/blog/2023/introduction-to-deep-reinforcement-learning/"><![CDATA[<style>.responsive-img{width:100%}@media(min-width:992px){.responsive-img{width:75%}}</style> <h2 id="artificial-intelligence-ai">Artificial Intelligence (AI)</h2> <ul> <li>Artificial Intelligence (AI) is a domain of computer science dedicated to developing software capable of exhibiting attributes of intelligence.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/subfields_of_artificial_intelligence.jpg" sizes="95vw"/> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/subfields_of_artificial_intelligence.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="machine-learning-ml">Machine Learning (ML)</h2> <ul> <li>Machine learning, a subset of AI, tackles problems necessitating intelligent solutions by learning from data.</li> <li><strong>Supervised Learning (SL):</strong> A method that learns from labeled data. <ul> <li>E.g., handwritten-digit-recognition</li> </ul> </li> <li><strong>Unsupervised Learning (UL):</strong> A method that learns from unlabeled data <ul> <li>E.g., customer segmentaiton</li> </ul> </li> <li><strong>Reinforcement Learning (RL):</strong> A method that learns from trial and error <ul> <li>E.g., pong-playing agent</li> </ul> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/main_branches_of_machine_learning.jpg" sizes="95vw"/> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/main_branches_of_machine_learning.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="deep-learning-dl">Deep Learning (DL)</h2> <ul> <li>Deep Learning employs multi-layered non-linear function approximations, also known as neural networks, to address ML tasks. Essentially, it is a suite of techniques that utilize neural networks to solve ML challenges.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/deep_learning_is_a_powerful_toolbox.jpg" sizes="95vw"/> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/deep_learning_is_a_powerful_toolbox.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="deep-reinforcement-learning-drl">Deep Reinforcement Learning (DRL)</h3> <ul> <li>Deep Reinforcement Learning learns through trial and error from feedback that’s simultaneously sequentially, evaluative, and sampled by leveraging non-linear function approximation (neural networks).</li> </ul> <h2 id="reinforcement-learning-rl">Reinforcement Learning (RL)</h2> <h3 id="similar-fields">Similar fields</h3> <ul> <li><strong>Reinforcement Learning (RL):</strong> Investigates methods of resolving complex sequential decision-making problems under uncertain conditions.</li> <li><strong>Control Theory (CT)</strong>: Examines methods of controlling complex known dynamic systems.</li> <li><strong>Operations Research (OR)</strong>: Investigates decision-making under uncertain conditions, generally featuring a larger action space than in DRL.</li> <li><strong>Psychology</strong>: Studies human behavior, which frequently encapsulates complex sequential decision-making problems under uncertainty.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/the_synergy_between_similar_fields.jpg" sizes="95vw"/> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/the_synergy_between_similar_fields.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="agent-and-enviroment">Agent and Enviroment</h3> <ul> <li> <p><strong>Agent:</strong> Refers exclusively to the decision-making entity.</p> <ul> <li>For instance, if you are training a robot arm to pick up a toy, the agent is the code that makes the decisions, not the robot arm itself.</li> </ul> </li> <li> <p><strong>Environment:</strong> Includes everything external to the agent, beyond the agent’s control, and everything that follows the agent’s decisions.</p> <ul> <li>In the context of training a robot arm to pick up a toy, the objects to be picked up, the tray where the objects reside, atmospheric conditions like wind, and even the robot arm itself are all considered parts of the environment.</li> </ul> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/boundary_between_agent_and_environment.jpg" sizes="95vw"/> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/boundary_between_agent_and_environment.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="states-and-observations">States and Observations</h3> <ul> <li><strong>State Space:</strong> The set of all possible variables and values that can represent the state of the environment.</li> <li><strong>State:</strong> A comprehensive description of the environment, or an instantiation of the state space.</li> <li><strong>Observation:</strong> A partial or incomplete description of the environment.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/states_vs_observations.jpg" sizes="95vw"/> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/states_vs_observations.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="reinforcement-learning-cycle">Reinforcement Learning Cycle</h3> <ul> <li><strong>Transition Function:</strong> The mapping from the agent’s action to a potentially new state.</li> <li><strong>Reward Function:</strong> The mapping from the action taken to the potential reward signal. <ul> <li>Goals are defined via the reward function.</li> </ul> </li> <li><strong>Model:</strong> A set of the transitions and rewards.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/the_reinforcement_learning_cycle.jpg" sizes="95vw"/> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/the_reinforcement_learning_cycle.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h4 id="agents-improvement-process">Agent’s Improvement process:</h4> <ol> <li>Interact with the environment.</li> <li>Evaluates its behavior.</li> <li>Improves its responses.</li> </ol> <h4 id="agents-can-be-designed-to-learn">Agent’s can be designed to learn:</h4> <ul> <li><strong>Policy:</strong> The mapping from observations to actions.</li> <li><strong>Models:</strong> A model of the environment on mappings.</li> <li><strong>Value Functions:</strong> The mapping of a state to its estimated value.</li> </ul> <h3 id="experiences">Experiences</h3> <ul> <li><strong>Time Step:</strong> A single cycle of interaction between the agent and the environment.</li> <li><strong>Experience:</strong> The set consisting of the state, the action, the reward, and the new state in a single time step.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/experience_tuples.jpg" sizes="95vw"/> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/experience_tuples.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li><strong>Episodic Task:</strong> Tasks that have a natural ending or goes on finitely many step. <ul> <li>E.g., video games</li> </ul> </li> <li><strong>Continuing Task:</strong> Tasks that don’t have a natural ending or could go on indefinitely. <ul> <li>E.g., learning forward motion</li> </ul> </li> </ul> <h3 id="credit-assignment-problem">Credit Assignment Problem</h3> <ul> <li><strong>Temporal Credit Assignment Problem:</strong> the challenge in determining which state and/or action is responsible for a reward the agent recieves <ul> <li>Usually occurs when the agent may have delayed rewards from an action or state that caused it hence the temporal aspect of the problem</li> </ul> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/temporal_credit_assignment_problem.jpg" sizes="95vw"/> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/temporal_credit_assignment_problem.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="exploration-vs-exploitation">Exploration vs. Exploitation</h3> <ul> <li><strong>Evaluative Feedback:</strong> Feedback that provides an indication of performance but not correctness.</li> <li><strong>Exploration versus Exploitation trade-off:</strong> The balance between collecting new information from the environment and using known information to maximize rewards.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/exploration_vs_exploitation.jpg" sizes="95vw"/> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/exploration_vs_exploitation.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="sampled-feedback">Sampled Feedback</h3> <ul> <li>Learning from sparse or weak feedback becomes more challenging with samples only. The agent must be capable of generalizing to learn from sampled feedback.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/learning_from_sampled_feedback.jpg" sizes="95vw"/> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/learning_from_sampled_feedback.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="types-of-agents">Types of Agents</h3> <ul> <li><strong>Policy-based:</strong> Designed to approximate policies.</li> <li><strong>Model-based:</strong> Designed to approximate models.</li> <li><strong>Value-based:</strong> Designed to approximate value functions.</li> <li><strong>Actor-critic:</strong> Designed to approximate both policies and value functions.</li> </ul> <h3 id="pros-and-cons">Pros and Cons</h3> <p><strong>Strength:</strong> Reinforcement learning excels in mastering specific tasks.</p> <p><strong>Weaknesses:</strong> To learn a well-performing policy, it generally requires millions of samples.</p> <h2 id="history-of-deep-reinforcement-learning">History of Deep Reinforcement Learning</h2> <ul> <li> <p><strong>Alan Turing - 1930:</strong> Developed the Turing Test, a test of a machine’s ability to exhibit intelligent behavior indistinguishable from that of a human.</p> </li> <li> <p><strong>John McCarthy - 1955:</strong> Coined the term “Artificial Intelligence”.</p> </li> <li> <p><strong>Andrew Ng - 2002:</strong> Trained an autonomous helicopter to perform stunts by observing human-expert flights using inverse reinforcement learning.</p> </li> <li> <p><strong>Nate Kohl and Peter Stone - 2002:</strong> Applied policy-gradient methods to train a soccer-playing robot.</p> </li> <li> <p><strong>Mnih et al. - 2013, 2015:</strong> Introduced the DQN algorithm, which learned to play Atari games from raw pixels.</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/introduction-to-deep-reinforcement-learning/atari_dqn.jpg" sizes="95vw"/> <img src="/assets/img/posts/introduction-to-deep-reinforcement-learning/atari_dqn.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>*</p> <ul> <li> <p><strong>Silver et al. - 2014:</strong> Released the deterministic policy gradient (DPG) algorithm.</p> </li> <li> <p><strong>Lillicrap et al. - 2015:</strong> Improved DPG with deep deterministic policy gradient (DDPG)</p> </li> <li> <p><strong>Schulman et al. - 2016:</strong> Released trust region policy optimization (TRPO) and generalized advantage estimation (GAE) methods.</p> </li> <li> <p><strong>Sergey Levine et al. - 2016:</strong> Published Guided Policy Search (GPS)</p> </li> <li> <p><strong>Silver et al. - 2016:</strong> Demonstrated AlphaGo</p> </li> <li> <p>…</p> </li> </ul> <h2 id="references">References</h2> <p>Morales, M. (2020). Grokking Deep Reinforcement Learning. Originally Published: October 15, 2020.</p> <p><em>All figures are sourced from this book.</em></p>]]></content><author><name>Ahad Jawaid</name></author><category term="Deep Reinforcement Learning"/><category term="Notes"/><summary type="html"><![CDATA[My notes on Deep Reinforcement Learning (DRL) based on the first chapter of the 'Grokking Deep Reinforcement Learning' by Miguel Morales.]]></summary></entry><entry><title type="html">From Paper to Code: A Guide to Implement Research</title><link href="https://www.ahadjawaid.com/blog/2023/translating-theory-into-code/" rel="alternate" type="text/html" title="From Paper to Code: A Guide to Implement Research"/><published>2023-06-05T00:00:00+00:00</published><updated>2023-06-05T00:00:00+00:00</updated><id>https://www.ahadjawaid.com/blog/2023/translating-theory-into-code</id><content type="html" xml:base="https://www.ahadjawaid.com/blog/2023/translating-theory-into-code/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Recently, I’ve been diving into the practice of implementing deep learning research papers. This post is designed to guide you through this process, sharing my key insights along the way.</p> <p>Starting with an independent study course at university, I explored deep learning intensively. To streamline the journey of understanding and implementing such papers, I’ve distilled the process into four steps:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/translating-theory-into-code/read_code_visualize_diagram.svg" sizes="95vw"/> <img src="/assets/img/posts/translating-theory-into-code/read_code_visualize_diagram.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Let’s break each one down.</p> <h2 id="read">Read</h2> <p>Reading a research paper should be focused on understanding the process rather than memorizing details. Pay attention to the inputs, outputs, data transformations, and assumptions.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/translating-theory-into-code/questions.svg" sizes="95vw"/> <img src="/assets/img/posts/translating-theory-into-code/questions.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To expedite the reading process, it helps to know where to find necessary details. Typically, the most crucial sections in a research paper are:</p> <ol> <li> <p><strong>Abstract</strong></p> <ul> <li>Provides a broad understanding of the paper and its relevance.</li> </ul> </li> <li> <p><strong>Figures</strong></p> <ul> <li>Often encapsulate the paper’s content and are valuable for understanding the model’s workings. However, ensure their accuracy by cross-referencing the textual content.</li> </ul> </li> <li> <p><strong>Introduction</strong></p> <ul> <li>Highlights the paper’s novelty or significance.</li> </ul> </li> <li> <p><strong>Architecture (Can go by other names)</strong></p> <ul> <li>Contains detailed descriptions of the architecture (usually where most of your time will be spent).</li> </ul> </li> <li> <p><strong>Training</strong></p> <ul> <li>Provides hyperparameters for training, model performance, and other essential information.</li> </ul> </li> <li> <p><strong>The appendix</strong></p> <ul> <li>Often contains implementation details not fitting into the main paper’s flow.</li> </ul> </li> <li> <p><strong>Experiements</strong> (Optional)</p> <ul> <li>Can include important data and parameters used for model training.</li> </ul> </li> </ol> <h2 id="implement">Implement</h2> <p>During implementation, the key is not to get overwhelmed by the complexity of the task. Instead, focus on the components you understand, and incrementally build upon them. It’s okay if your code isn’t flawless initially; revisions can be made as you progress.</p> <h2 id="visualize">Visualize</h2> <p>Post-implementation, validate your code. Use libraries like <a href="https://matplotlib.org/">matplotlib</a> to visualize the outputs, or tools like <a href="https://docs.pytest.org/en/7.3.x/">pytest</a> to perform tests. Working in an interactive environment like a <a href="https://jupyter.org/">jupyter notebook</a> can also be helpful.</p> <h2 id="repeat">Repeat</h2> <p>The key to successful implementation is repetition. Even if you start slowly, your pace will increase as you build your knowledge and develop better abstractions. This process also improves your programming skills.</p> <h2 id="conclusion">Conclusion</h2> <p>In summary, the methodology I formulated and the key lessons I learned during my initial experience implementing a research paper can be encapsulated in four words: read, implement, visualize, and repeat. I hope you find this post beneficial, and I welcome any feedback or suggestions you might have. Thank you!</p>]]></content><author><name>Ahad Jawaid</name></author><category term="Research"/><summary type="html"><![CDATA[This guide covers effective reading, model implementation, code validation, and the power of repetition.]]></summary></entry><entry><title type="html">Neural network: The universal function</title><link href="https://www.ahadjawaid.com/blog/2023/the-universal-function/" rel="alternate" type="text/html" title="Neural network: The universal function"/><published>2023-02-23T00:00:00+00:00</published><updated>2023-02-23T00:00:00+00:00</updated><id>https://www.ahadjawaid.com/blog/2023/the-universal-function</id><content type="html" xml:base="https://www.ahadjawaid.com/blog/2023/the-universal-function/"><![CDATA[<p>Have you ever imagined a single machine that can adapt to any role given to it? It could be a screw, a tire, a screen or any other task you could think of. That’s what a neural network is for software — a function that can perform any task.</p> <p>Initially, when I learned about neural networks, it seemed like a highly abstract concept that emulated the brain. However, after reading books such as Deep Learning by Ian Goodfellow and The Deep Learning practical coder by Jeremy Howard, I came to realize that a neural network is nothing more than a function that can be adjusted to perform any task we require of it.</p> <h2 id="what-exactly-is-a-function">What exactly is a function?</h2> <p>A function is a mapping of an input to an output. For instance, an addition function could take two numbers as inputs and output their sum.</p> <h2 id="adjustable-function">Adjustable function</h2> <p>An adjustable function, on the other hand, is a function that has weights or parameters that regulate how the inputs are modified to generate a specific output. For example, a function f that takes two numbers as inputs can be expressed as</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">f(input1, input2) = weight1 * input1 + weight2 * input2</code></p> </blockquote> <p>Here, we can change the function by adjusting the weights.</p> <h2 id="how-can-an-adjustable-function-learn-a-task">How can an adjustable function learn a task?</h2> <p>Now that we have established what an adjustable function is, let’s look at how we can use it to solve a problem. The process of adjusting the weights in the function to accomplish a task involves three main steps:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. Get output from the function
2. Check how wrong the output is compared to 3. the desired output
4. Adjust the weights to make the output look more like the desired output
</code></pre></div></div> <p>This process can be illustrated in the following diagram:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/the-universal-function/model.png" sizes="95vw"/> <img src="/assets/img/posts/the-universal-function/model.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>As simple as this may seem, this is the core of how a neural network learns a task. It starts by guessing an output, and then we adjust the weights to make it look more like the desired output.</p> <h2 id="adjusting-the-weights-automatically">Adjusting the weights automatically</h2> <p>Adjusting the weights automatically is where it gets interesting. Let’s assume you’re playing a game where someone has a number in mind, and you have to guess it. Every time you make a guess, they tell you whether you’re close or far. If you randomly pick the numbers at random, you’d get nowhere. But adding two simple steps to the process can solve this problem:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. A method to determine how far you are from the desired output (output error method)
2.A method for finding the direction and extent of the weights to be changed (method for determining how to adjust weights)
</code></pre></div></div> <h2 id="output-error-method">Output Error Method</h2> <p>To find how far we are from the desired output, we can subtract our function’s output and the desired output,</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">f(input1, input2) = 5</code> <code class="language-plaintext highlighter-rouge">goal_function(input1, input2) = 10</code> <code class="language-plaintext highlighter-rouge">error = 5–10 = -5</code></p> </blockquote> <p>The problem with this method is that it could give us negative values, which can be problematic when attempting to minimize the error. To address this issue, we can bound the method by taking the absolute value of the subtraction (positive number).</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">error = | 5–10 | = 5</code></p> </blockquote> <h2 id="method-for-determining-how-to-adjust-weights">Method for determining how to adjust weights</h2> <p>The last addition to adjust the weights automatically is to find the direction and extent of the weights to be changed. We can accomplish this by using a concept from calculus known as the derivative, which is the slope of the function at a specific point. By finding the slope or derivative, we can determine the direction and amount to change the weights.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/the-universal-function/tanget.png" sizes="95vw"/> <img src="/assets/img/posts/the-universal-function/tanget.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>So in this illustration, we can find that the slope at point one is two by using the rise-over run of the tangent line (a line that touches the function at a point). We can use this slope or derivative to determine in what direction and how much to change the weights. This is done because of a property that a derivative has, which is when the derivative is zero, it is at the minimum or maximum (or saddle point) of the function. We can take advantage of this property with our method of measuring the error to find the minimum of the error.</p> <p>So if we keep adjusting the weights and the derivative gets smaller, we are approaching a minimum or maximum (or saddle point). To ensure we are finding the minimum and not the maximum error, we need to figure out what direction we should change the weights. This can be done simply by subtracting the derivative to act as we descend to the minimum. Putting it together</p> <p>The heart of deep learning lies in the ability of neural networks to learn any task by going through five steps. These are:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. Input data into the function
2. Compare the output to the error function (loss function)
3. Take the derivative of the error function with respect to the weights
4. Subtract the derivative of the weights from the weights
5. Repeat until the error is small
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/the-universal-function/universal-function.png" sizes="95vw"/> <img src="/assets/img/posts/the-universal-function/universal-function.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Everything else is focused on making the training process efficient and timely.</p> <h2 id="the-universal-function">The Universal Function</h2> <p>In conclusion, the neural network is a powerful tool that allows software to adapt to any role given to them. At first glance, it may seem like an abstract concept, but it is nothing more guessing and checking then improving. What’s fascinating about this is that the improvement can be made automatically by using concepts from calculus like the derivative, which allows us to determine how to improve. With these tools, the neural network can learn any task and solve most problem thrown its way.</p> <p>Today, we see the applications of neural networks in various fields, including speech recognition, image and pattern recognition, and natural language processing, to name a few. The possibilities of what we can achieve with this technology are endless, and we are only scratching the surface of what it can do.</p>]]></content><author><name>Ahad Jawaid</name></author><category term="Deep Learning"/><category term="Neueral Networks"/><summary type="html"><![CDATA[Have you ever imagined a single machine that can adapt to any role given to it? It could be a screw, a tire, a screen or any other task you could think of. That’s what a neural network is for software — a function that can perform any task.]]></summary></entry></feed>